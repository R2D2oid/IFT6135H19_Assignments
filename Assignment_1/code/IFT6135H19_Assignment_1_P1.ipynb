{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please run MNIST_download.ipynb to download, pre-process, and store all the necessary data in mnist_dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W7UoFBoCFPKA",
    "outputId": "6af02037-73fe-4983-dd01-e57581659594"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IFT6135H19_A1_Pc_no_change.png\t  mnist_dataset\r\n",
      "IFT6135H19_A1_Pc.png\t\t  MNIST_download.ipynb\r\n",
      "IFT6135H19_Assignment_1_P1.ipynb  mnist_experiments\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFgiqz8yYmng"
   },
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/agoose77/numpy-html.git#egg=numpy-html\n",
    "#import numpy_html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "#np.set_printoptions(threshold=5, edgeitems=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1,2,3) Multi-Layer Perceptron in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Helper Function for Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Affine Layer Forward and Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aYky0piO1K9T"
   },
   "outputs": [],
   "source": [
    "def Affine_forward(inp, W, B):\n",
    "  # Params, inp: Input to Layer : (NxD)\n",
    "  #         W: weight of Layer  : (DxM)\n",
    "  #         B  Bias of Layer    : (1xM)\n",
    "  # Output, out = inp*W + B     : (NxM) \n",
    "\n",
    "  out = np.dot(inp,W) + B  # out: (NxD)x(D,M) + (1,M) = (N,M)\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zk1CumTAx7mo"
   },
   "outputs": [],
   "source": [
    "def Affine_backward(inp, W, B, gradient):\n",
    "  # Assume, inp                         : (NxD)\n",
    "  #         W                           : (DxM)\n",
    "  #         B                           : (1XM)\n",
    "  #         gradient                    : (NxM)\n",
    "  #         reg regulazier scaler       : (1x1)  \n",
    "  # Output, Dinp = gradient*Traspose(T) : (NxD)\n",
    "  #         DW = Traspose(inp)*gradient : (NxD)\n",
    "  #         DB = Sum_N gradient         : (1xM) \n",
    "  \n",
    "  Dinp = np.dot(gradient, W.T)                  # DH: (NxM) * (MxD) = (NxD)\n",
    "  DW = np.dot(inp.T, gradient)                # DW  : (DxN) * (NxM) = (DxM)\n",
    "  DB = np.sum(gradient, 0, keepdims=True)     # DB  : Sum_N (N,M)   = (1xM)\n",
    "    \n",
    "  return Dinp, DW, DB\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) ReLU forward and backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "57WN3A_9l6z5"
   },
   "outputs": [],
   "source": [
    "def ReLU(inp):\n",
    "  # Params, inp               : (N,M)\n",
    "  # Output, activ = max(0,inp): (N,M)\n",
    "\n",
    "  activ = np.maximum(0,inp)\n",
    "  \n",
    "  return np.maximum(0,inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x78eTYjMuOCt"
   },
   "outputs": [],
   "source": [
    "def ReLU_backward(inp, gradient):\n",
    "  # Params, inp: Input to ReLU                                             : (N,M)\n",
    "  #         gradient: Backpropogating gradient from the next layer         : (N,M)\n",
    "  # Output: gradient: gradient * Derivative of ReLU (1 if X>0 otherwise 0) : (N,M)\n",
    "  \n",
    "  gradient[inp<=0] = 0\n",
    "  \n",
    "  return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Sigmoid Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DE7no9RdnHdI"
   },
   "outputs": [],
   "source": [
    "def Sigmoid(inp):\n",
    "  # Params, inp                       : (N,M)\n",
    "  # Output, activ = 1 / 1 + exp(-inp) : (N,M)\n",
    "  \n",
    "  activ = 1 / (1 + np.exp(-inp))\n",
    "  \n",
    "  return activ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDKs21WMnsB2"
   },
   "outputs": [],
   "source": [
    "def Sigmoid_backward(inp, gradient):\n",
    "  # Params, inp: Input to ReLU                                                 : (N,M)\n",
    "  #         gradient: Backpropogating gradient from the next layer             : (N,M)\n",
    "  # Output: gradient: gradient * Derivative of Sigmoid (sigmoid * (1-sigmoid)) : (N,M)\n",
    " \n",
    "  s = Sigmoid(inp)  \n",
    "  Dsigmoid = s*(1-s)\n",
    "  \n",
    "  out = gradient * Dsigmoid\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) TanH Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tL9tpCgpN9B"
   },
   "outputs": [],
   "source": [
    "def TanH(inp):\n",
    "  # Params, inp                                         : (N,M)\n",
    "  # Output, activ = (exp(2*inp) - 1) / (exp(2*inp) + 1) : (N,M)\n",
    "  \n",
    "  exp2a = np.exp(2*inp)\n",
    "  activ = (exp2a - 1) / (exp2a + 1)\n",
    "  \n",
    "  return activ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmWtJENpqb5U"
   },
   "outputs": [],
   "source": [
    "def TanH_backward(inp, gradient):\n",
    "  # Params, inp: Input to TanH                                                 : (N,M)\n",
    "  #         gradient: Backpropogating gradient from the next layer             : (N,M)\n",
    "  # Output: gradient: gradient * Derivative of TanH (1-TanH^2))                : (N,M)\n",
    "\n",
    "  tanh = TanH(inp)\n",
    "  Dtanh = 1 - (tanh**2)\n",
    "  \n",
    "  out = gradient * Dtanh\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) SoftMax Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlZAXV3SlMoS"
   },
   "outputs": [],
   "source": [
    "def softmax(inp):\n",
    "  # Params, inp                                        : (N,C)\n",
    "  # Output, out: probs = exp(inp_i) / Sum_j exp(inp_j) : (N,C)\n",
    "    \n",
    "  exp_inp = np.exp(inp)\n",
    "  probs = exp_inp / np.sum(exp_inp, axis=1, keepdims=True)\n",
    "  \n",
    "  return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yBV9l8ifh0Ja"
   },
   "outputs": [],
   "source": [
    "def softmax_backward(probs, GT):\n",
    "  # Params, probs: Network output probabilities                                                : (N,C)\n",
    "  #         GT:  each value belongs to one of C classes                                        : (N,)\n",
    "  # Output, Gradient of Pre-SoftMax activation with respect to output (dout = -(e(y) - probs)) : (N,C)          \n",
    "\n",
    "  dout = probs\n",
    "  dout[range(GT.shape[0]),GT] -= 1\n",
    "  dout /= GT.shape[0]\n",
    "\n",
    "  return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9I66WVX3g8jc"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(probs, GT):\n",
    "  # Params, probs: (N,C) where sum_c = 1\n",
    "  #         GT   : (N,) \n",
    "  # Output, loss : scalar\n",
    "  \n",
    "  # compute log probability for true value of GT for each example\n",
    "  logprobs = -np.log(probs[range(GT.shape[0]),GT])\n",
    "\n",
    "  loss = np.sum(logprobs)\n",
    "  loss /= GT.shape[0]\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Function to shuffle training data after epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fAwhNjj-apAv"
   },
   "outputs": [],
   "source": [
    "def shuffle_data(X,Y):\n",
    "   \n",
    "  s = np.arange(X.shape[0])\n",
    "  np.random.shuffle(s)\n",
    "\n",
    "  return X[s], Y[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Saving and Loading python objects as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S8WlMQ7lUufq"
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, path, name):\n",
    "    with open(path + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3NGOlFkVJAL"
   },
   "outputs": [],
   "source": [
    "def load_obj(path, name):\n",
    "    with open(path + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Neural Network Class Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that NN class has a function named initialization which allows us to initialize NN with any initialization method (glorot, normal, or zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fs9lThS8rZVH"
   },
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "\n",
    "  \n",
    "  def __init__(self,\n",
    "               input_dims=784, \n",
    "               output_dims=10, \n",
    "               hidden_dims=(1024,2048), \n",
    "               n_hidden=2, \n",
    "               regularization_coefficient=0,\n",
    "               init_method='zeros',           # 'zeros', 'normal', or 'glorot'\n",
    "               nonlinearity = 'relu',         # 'relu', 'tanh', or 'sigmoid'\n",
    "               LearningRate = 0.001):\n",
    "    \n",
    "    dims = (input_dims,) + hidden_dims + (output_dims,)    \n",
    "    self.n_hidden = n_hidden\n",
    "     \n",
    "    self.LR = LearningRate\n",
    "    self.reg_coeff = regularization_coefficient\n",
    "    \n",
    "    self.params = {}    # dictionary of parameters of NN\n",
    "    self.cache = {}     # dictionary of forward model cache\n",
    "    self.gradient = {}  # dictionary of backward model cache (gradient)\n",
    "\n",
    "    self.activ = nonlinearity\n",
    "    self.initialize_weights(n_hidden, dims, init_method)\n",
    "         \n",
    "        \n",
    "  def initialize_weights(self,n_hidden, dims, init_method):\n",
    "    \n",
    "    # W = weight parameters\n",
    "    # B = Biases\n",
    "    \n",
    "    for i in range(1,n_hidden+2):\n",
    "      \n",
    "      self.params['B'+str(i)] = np.zeros((1,dims[i]))\n",
    "      \n",
    "      if init_method == 'zeros':\n",
    "        self.params['W'+str(i)] = np.zeros((dims[i-1],dims[i]))\n",
    "      elif init_method == 'normal':\n",
    "        self.params['W'+str(i)] = np.random.normal(0.0, 1.0, (dims[i-1],dims[i]))        \n",
    "      elif init_method == 'glorot':\n",
    "        dl = np.sqrt(6/(dims[i-1]+dims[i]))\n",
    "        self.params['W'+str(i)] = np.random.uniform(-dl, dl, (dims[i-1],dims[i]))\n",
    "      else:\n",
    "        raise Exception('Weight Intialization Method should be one of the following: zeros, glorot, or normal') \n",
    "    \n",
    "    # DA = gradient of W\n",
    "    # DB = gradient pf B\n",
    "    for i in range(1,n_hidden+2):\n",
    "\n",
    "      self.gradient['DB'+str(i)] = np.zeros((1,dims[i]))     \n",
    "      self.gradient['DW'+str(i)] = np.zeros((dims[i-1],dims[i]))\n",
    "\n",
    "        \n",
    "  def activation(self,input):\n",
    "    \n",
    "    if self.activ == 'relu':\n",
    "      out = ReLU(input)\n",
    "    elif self.activ == 'tanh':\n",
    "      out = TanH(input)\n",
    "    elif self.activ == 'sigmoid':\n",
    "      out = Sigmoid(input)\n",
    "    else:\n",
    "      raise Exception('NonLinearity should be one of the following: relu, tanh, or sigmoid')\n",
    "        \n",
    "    return out\n",
    "        \n",
    "\n",
    "  def activation_backward(self,input,gradient):\n",
    "    \n",
    "    if self.activ == 'relu':\n",
    "      out = ReLU_backward(input, gradient)\n",
    "    elif self.activ == 'tanh':\n",
    "      out = TanH_backward(input, gradient)\n",
    "    elif self.activ == 'sigmoid':\n",
    "      out = Sigmoid_backward(input, gradient)\n",
    "    else:\n",
    "      raise Exception('NonLinearity should be one of the following: relu, tanh, or sigmoid')\n",
    "        \n",
    "    return out\n",
    "\n",
    "  \n",
    "  def forward(self,input):    \n",
    "    \n",
    "    # W = weight parameters\n",
    "    # B = Biases\n",
    "    # A = pre-activation (affine transformed input)\n",
    "    # H = post-activation (Hidden layer output)\n",
    "\n",
    "    self.cache['input'] = input\n",
    "    self.cache['A1'] = Affine_forward(self.cache['input'], self.params['W1'], self.params['B1']) # A1 = X*W1 + B1\n",
    "\n",
    "    for i in range(1,self.n_hidden+1):\n",
    "      self.cache['H'+str(i)] = self.activation(self.cache['A'+str(i)]) # Hi = active(Ai), ex: H1=active(A1)\n",
    "      self.cache['A'+str(i+1)] = Affine_forward(self.cache['H'+str(i)], self.params['W'+str(i+1)], self.params['B'+str(i+1)]) # A_(i+1) = H_(i)*W_(i+1) + B_(i+1), ex: A2 = H1*W2 + B2\n",
    "\n",
    "    self.cache['out'] = softmax(self.cache['A'+str(self.n_hidden+1)]) # out = softmax(A3)\n",
    "      \n",
    "    return self.cache['out']\n",
    "\n",
    "  \n",
    "  def loss(self,prediction,labels):\n",
    "\n",
    "    data_loss = cross_entropy_loss(prediction, labels)\n",
    "    \n",
    "    reg_loss = 0\n",
    "    \n",
    "    # if regularization parameter if greater than 0 than calculate L2 value of all weight layer to calculate reg_loss\n",
    "    if self.reg_coeff>0:\n",
    "      reg_loss = sum(np.sum(self.params['W'+str(i)]**2) for i in range(1,self.n_hidden+2))\n",
    "\n",
    "    loss = data_loss + (self.reg_coeff * reg_loss)\n",
    "    \n",
    "    return loss\n",
    "  \n",
    "  \n",
    "  def backward(self, labels):\n",
    "    \n",
    "    # DA is a dummy variable to store gradient of pre-activation\n",
    "    # DH is a dummy variable to store gradient of post-pactivation\n",
    "\n",
    "    DA = softmax_backward(self.cache['out'], labels) \n",
    "    \n",
    "    for i in range(self.n_hidden+1, 1, -1):\n",
    "      DH, self.gradient['DW'+str(i)], self.gradient['DB'+str(i)] =  Affine_backward(self.cache['H'+str(i-1)], self.params['W'+str(i)], self.params['B'+str(i)], DA) # DH2, DW3, DB3 = backward(H2, W3, B3)\n",
    "      DA = self.activation_backward(self.cache['A'+str(i-1)], DH) #DA2 = backward(A2, DH2)\n",
    " \n",
    "    _, self.gradient['DW1'], self.gradient['DB1'] = Affine_backward(self.cache['input'], self.params['W1'], self.params['B1'], DA) # Dinp, DW1, DB1 = backward(input, W1, B1, DA1)\n",
    "\n",
    "     \n",
    "  def update(self):\n",
    "    \n",
    "    # Update NN parameters\n",
    "    \n",
    "    for i in range(1,self.n_hidden+2):\n",
    "\n",
    "      self.params['B'+str(i)] -= self.LR*self.gradient['DB'+str(i)] \n",
    "      \n",
    "      if self.reg_coeff>0:\n",
    "        self.gradient['DW'+str(i)] += self.reg_coeff * self.params['W'+str(i)]\n",
    "\n",
    "      self.params['W'+str(i)] -= self.LR*self.gradient['DW'+str(i)]\n",
    "  \n",
    "  \n",
    "  def zero_gradient(self):\n",
    "    \n",
    "    # make gradients zero\n",
    "    # useful to do it at the starting of epocj\n",
    "\n",
    "    for i in range(1,n_hidden+2):\n",
    "\n",
    "      self.gradient['DB'+str(i)] = np.zeros(self.gradient['DB'+str(i)].shape)     \n",
    "      self.gradient['DW'+str(i)] = np.zeros(self.gradient['DW'+str(i)].shape)\n",
    "      \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function to train model for one epoch (pass through all training images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcCJ5vNQbQse"
   },
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, batch_size):\n",
    "    \n",
    "  samples = x_train.shape[0]\n",
    "\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for i in range(0, samples, batch_size):\n",
    "      \n",
    "    data, label = x_train[i:i+batch_size], y_train[i:i+batch_size] # get batch of data and label\n",
    "      \n",
    "    pred_probs = model.forward(data) # forward pass of the data\n",
    "      \n",
    "    batch_loss = model.loss(pred_probs, label) # calculate loss with respect to label\n",
    "      \n",
    "    epoch_loss += batch_loss # increment epoch loss\n",
    "      \n",
    "    model.backward(label) # backward pass with respect to label\n",
    "      \n",
    "    model.update() # update model parameters\n",
    "      \n",
    "    model.zero_gradient() # make gradient zero\n",
    "      \n",
    "  epoch_loss = (epoch_loss * batch_size) / (samples) # normalize epoch loss with respect to batch size and total samples\n",
    "    \n",
    "  return epoch_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function to get prediction for all test data and calculate loss and accuracy for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SeptjN7t9j6V"
   },
   "outputs": [],
   "source": [
    "def test(model, x, y):\n",
    "\n",
    "  model.zero_gradient() # make model gradient to zero\n",
    "  \n",
    "  samples = x.shape[0] # calculate total data samples\n",
    "  \n",
    "  epoch_loss = 0       # to store loss for the wholte dataset\n",
    "  true_prediction = 0  # to store total true prediction\n",
    "  \n",
    "  for i in range(0, samples):\n",
    "      \n",
    "    data, label = x[i:i+1], y[i:i+1] # get a sample of data\n",
    "      \n",
    "    pred_probs = model.forward(data) # forward pass through model\n",
    "    \n",
    "    pred_labels = np.argmax(pred_probs) # find the predicted label\n",
    "    \n",
    "    batch_loss = model.loss(pred_probs, label) # calculate loss\n",
    "\n",
    "    true_prediction += np.count_nonzero(pred_labels-label == 0) # check if prediction and true label are same or not\n",
    "    \n",
    "    epoch_loss += batch_loss # increment epoch loss\n",
    "            \n",
    "  epoch_loss = epoch_loss / samples # normalize loss with batch_size and samples\n",
    "  \n",
    "  accuracy = true_prediction / samples # calculate accuracy\n",
    "  \n",
    "  return epoch_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QG9ugMXk1p9y"
   },
   "outputs": [],
   "source": [
    "# experiment parameters\n",
    "total_epoch = 10\n",
    "\n",
    "data_path = 'mnist_dataset/mnist.npy'\n",
    "model_path = 'mnist_experiments/Exp_validate_gradient/'\n",
    "\n",
    "os.makedirs(model_path,exist_ok=True)\n",
    "\n",
    "input_dims=784 \n",
    "output_dims=10 \n",
    "hidden_dims=(64,32) \n",
    "n_hidden=2 \n",
    "regularization_coefficient=0\n",
    "init_method ='glorot'          # 'zeros', 'normal', or 'glorot'\n",
    "nonlinearity = 'relu'         # 'relu', 'tanh', or 'sigmoid'\n",
    "LearningRate = 0.1\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read MNIST data and pre-process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rK2y5aiy4_ff"
   },
   "outputs": [],
   "source": [
    "# data-preparation\n",
    "(tr, va, te) = np.load(data_path)\n",
    "    \n",
    "x_train = tr[0]\n",
    "y_train = tr[1]\n",
    "x_valid = va[0]\n",
    "y_valid = va[1]\n",
    "x_test  = te[0]\n",
    "y_test  = te[1]\n",
    "    \n",
    "data_mean = x_train.mean()\n",
    "data_std  = x_train.std()\n",
    "    \n",
    "x_train = (x_train - data_mean) / data_std \n",
    "x_test  = (x_test  - data_mean) / data_std \n",
    "x_valid = (x_valid - data_mean) / data_std \n",
    "\n",
    "del tr, va, te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D8OJ0Ycs6tyd",
    "outputId": "f9f4c680-ed0e-4639-d0cb-9f2029c28e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Model Parameters:  52650\n"
     ]
    }
   ],
   "source": [
    "model = NN(input_dims, output_dims, hidden_dims, n_hidden, regularization_coefficient, init_method, nonlinearity, LearningRate)\n",
    "\n",
    "model_params = sum(value.size for _, value in model.params.items())\n",
    "\n",
    "print('Total Model Parameters: ', model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network for total_epochs and print and store training and validation loss and accuracy after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "3X4YHiBqBIOA",
    "outputId": "a6c1709d-a301-4bc3-c6c9-3d8ee316d2e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Training Done Epoch 1\n",
      "===> Training   Epoch 1: Loss - 0.1661, Accuracy - 0.9492\n",
      "===> Validation Epoch 1: Loss - 0.1822, Accuracy - 0.9487\n",
      "===> Model Saved Epoch 1\n",
      "===> Training Done Epoch 2\n",
      "===> Training   Epoch 2: Loss - 0.1203, Accuracy - 0.9615\n",
      "===> Validation Epoch 2: Loss - 0.1519, Accuracy - 0.9564\n",
      "===> Model Saved Epoch 2\n",
      "===> Training Done Epoch 3\n",
      "===> Training   Epoch 3: Loss - 0.0895, Accuracy - 0.9719\n",
      "===> Validation Epoch 3: Loss - 0.1317, Accuracy - 0.9650\n",
      "===> Model Saved Epoch 3\n",
      "===> Training Done Epoch 4\n",
      "===> Training   Epoch 4: Loss - 0.0945, Accuracy - 0.9713\n",
      "===> Validation Epoch 4: Loss - 0.1543, Accuracy - 0.9635\n",
      "===> Model Saved Epoch 4\n",
      "===> Training Done Epoch 5\n",
      "===> Training   Epoch 5: Loss - 0.0863, Accuracy - 0.9742\n",
      "===> Validation Epoch 5: Loss - 0.1600, Accuracy - 0.9638\n",
      "===> Model Saved Epoch 5\n",
      "===> Training Done Epoch 6\n",
      "===> Training   Epoch 6: Loss - 0.0767, Accuracy - 0.9773\n",
      "===> Validation Epoch 6: Loss - 0.1499, Accuracy - 0.9653\n",
      "===> Model Saved Epoch 6\n",
      "===> Training Done Epoch 7\n",
      "===> Training   Epoch 7: Loss - 0.0538, Accuracy - 0.9834\n",
      "===> Validation Epoch 7: Loss - 0.1404, Accuracy - 0.9683\n",
      "===> Model Saved Epoch 7\n",
      "===> Training Done Epoch 8\n",
      "===> Training   Epoch 8: Loss - 0.0552, Accuracy - 0.9830\n",
      "===> Validation Epoch 8: Loss - 0.1505, Accuracy - 0.9699\n",
      "===> Model Saved Epoch 8\n",
      "===> Training Done Epoch 9\n",
      "===> Training   Epoch 9: Loss - 0.1034, Accuracy - 0.9705\n",
      "===> Validation Epoch 9: Loss - 0.2037, Accuracy - 0.9579\n",
      "===> Model Saved Epoch 9\n",
      "===> Training Done Epoch 10\n",
      "===> Training   Epoch 10: Loss - 0.0585, Accuracy - 0.9825\n",
      "===> Validation Epoch 10: Loss - 0.1586, Accuracy - 0.9680\n",
      "===> Model Saved Epoch 10\n",
      "===> Testing Accuracy at the end of training: 0.9640\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "  \n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for i in range(1,total_epoch+1):\n",
    "  \n",
    "  loss = train(model, x_train, y_train, batch_size)\n",
    "  \n",
    "  print(\"===> Training Done Epoch {}\".format(i))\n",
    "  \n",
    "  loss, accuracy = test(model, x_train, y_train)\n",
    "  \n",
    "  train_loss = np.concatenate((train_loss, [loss]))\n",
    "  train_accuracy = np.concatenate((train_accuracy, [accuracy]))\n",
    "\n",
    "  print(\"===> Training   Epoch {}: Loss - {:.4f}, Accuracy - {:.4f}\".format(i, train_loss[-1], train_accuracy[-1]))\n",
    "\n",
    "  loss, accuracy = test(model, x_valid, y_valid)\n",
    "  \n",
    "  valid_loss = np.concatenate((valid_loss, [loss]))\n",
    "  valid_accuracy = np.concatenate((valid_accuracy, [accuracy]))\n",
    "\n",
    "  print(\"===> Validation Epoch {}: Loss - {:.4f}, Accuracy - {:.4f}\".format(i, valid_loss[-1], valid_accuracy[-1]))\n",
    "  \n",
    "  save_obj(model.params, model_path, 'model-{:02d}'.format(i))\n",
    "  \n",
    "  print(\"===> Model Saved Epoch {}\".format(i))\n",
    "\n",
    "  shuffle_data(x_train, y_train)\n",
    "\n",
    "  \n",
    "save_obj(train_loss, model_path, 'train-loss')\n",
    "save_obj(train_accuracy, model_path, 'train-accuracy')\n",
    "save_obj(valid_loss, model_path, 'valid-loss')\n",
    "save_obj(valid_accuracy, model_path, 'valid-accuracy')\n",
    "\n",
    "\n",
    "_, test_accuracy = test(model, x_test, y_test)\n",
    "\n",
    "print(\"===> Testing Accuracy at the end of training: {:.4f}\".format(test_accuracy) )\n",
    "\n",
    "save_obj([test_accuracy], model_path, 'test-accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4) Validate Gradient using finite difference at the end of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a single training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = x_train[1:2], y_train[1:2]\n",
    "      \n",
    "pred_probs = model.forward(data)\n",
    "                  \n",
    "model.backward(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store original params W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_W2 = model.params['W2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Maximum valur of i, j, and K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_i = 10 # first 10-elements of second layer weight\n",
    "max_j = 6 # j = {0,1,2,3,4,5}\n",
    "max_k = 5 # k = {1,2,3,4,5}\n",
    "\n",
    "Approx_delta = np.zeros((max_i, max_j, max_k)) #(10,6,5)\n",
    "\n",
    "True_delta = model.gradient['DW2'][30][10:10+max_i]\n",
    "True_delta = np.repeat(True_delta[:,np.newaxis],max_j,axis=-1)\n",
    "True_delta = np.repeat(True_delta[:,:,np.newaxis],max_k,axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00 -3.19575714e-06  3.61958291e-06  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.52373461e-08  0.00000000e+00\n",
      " -3.23730575e-06  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(model.gradient['DW2'][30][10:10+max_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### approximate gradient with finite difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,max_i):\n",
    "    for j in range(0,max_j):\n",
    "        for k in range(0,max_k):\n",
    "            N = (k+1)*(10**j)\n",
    "            epsi = 1 / N\n",
    "            ## positive\n",
    "            model.params['W2'][30][10+i] += epsi\n",
    "            pred_probs = model.forward(data)\n",
    "            loss = model.loss(pred_probs, label)\n",
    "            pos = loss\n",
    "            model.params['W2'] = orig_W2\n",
    "            ## negative\n",
    "            model.params['W2'][30][10+i] -= epsi\n",
    "            pred_probs = model.forward(data)\n",
    "            loss = model.loss(pred_probs, label)\n",
    "            neg = loss\n",
    "            model.params['W2'] = orig_W2\n",
    "            ## Approximate Delta\n",
    "            Approx_delta[i][j][k] = (pos-neg) / (2*epsi)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate maximum difference between finite difference and true difference for difference values of N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.zeros((max_j*max_k,))\n",
    "max_diff = np.zeros((max_j*max_k,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.e+00 2.e+00 3.e+00 4.e+00 5.e+00 1.e+01 2.e+01 3.e+01 4.e+01 5.e+01\n",
      " 1.e+02 2.e+02 3.e+02 4.e+02 5.e+02 1.e+03 2.e+03 3.e+03 4.e+03 5.e+03\n",
      " 1.e+04 2.e+04 3.e+04 4.e+04 5.e+04 1.e+05 2.e+05 3.e+05 4.e+05 5.e+05]\n",
      "[1.90354837e-08 1.01766688e-06 1.32309359e-06 1.45896441e-06\n",
      " 1.53563260e-06 1.67888615e-06 1.74580454e-06 1.76745189e-06\n",
      " 1.77815516e-06 1.78453898e-06 1.79722149e-06 1.80352048e-06\n",
      " 1.80561387e-06 1.80665944e-06 1.80728638e-06 1.80853949e-06\n",
      " 1.80916527e-06 1.80937400e-06 1.80947891e-06 1.80954081e-06\n",
      " 1.80966654e-06 1.80972760e-06 1.80974703e-06 1.80975869e-06\n",
      " 1.80976424e-06 1.80976424e-06 1.80978089e-06 1.80979200e-06\n",
      " 1.80978089e-06 1.80974759e-06]\n"
     ]
    }
   ],
   "source": [
    "#print(Approx_delta[9][0][0])\n",
    "#print(True_delta[9][0][0])\n",
    "i = 0\n",
    "for j in range(max_j):\n",
    "    for k in range(max_k):\n",
    "        N[i] = (k+1)*(10**j)\n",
    "        max_diff[i] = np.max(True_delta[:,j,k] - Approx_delta[:,j,k])\n",
    "        i = i+1\n",
    "    \n",
    "\n",
    "print(N)\n",
    "print(max_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot this value as a function of N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(0)\n",
    "plt.semilogx(N, max_diff)\n",
    "#plt.title('Maximum Difference between the true gradient and the finite gradient as a function of N')\n",
    "plt.grid(True)\n",
    "plt.xlabel('N=(k)*(10^j) where k={1,...,5}, j={0,...,5}')\n",
    "plt.ylabel('max_{1<=i<=10} |Delta_i^N - DL/DT_i|')\n",
    "#plt.show()\n",
    "plt.savefig('IFT6135H19_A1_Pc.png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[-3.19575714e-06 -3.19575714e-06 -3.19575714e-06 -3.19575714e-06\n",
      "   -3.19575714e-06]\n",
      "  [-3.19575714e-06 -3.19575714e-06 -3.19575714e-06 -3.19575714e-06\n",
      "   -3.19575714e-06]\n",
      "  [-3.19575714e-06 -3.19575714e-06 -3.19575714e-06 -3.19575714e-06\n",
      "   -3.19575714e-06]\n",
      "  [-3.19575714e-06 -3.19575714e-06 -3.19575714e-06 -3.19575714e-06\n",
      "   -3.19575714e-06]\n",
      "  [-3.19575714e-06 -3.19575714e-06 -3.19575714e-06 -3.19575714e-06\n",
      "   -3.19575714e-06]\n",
      "  [-3.19575714e-06 -3.19575714e-06 -3.19575714e-06 -3.19575714e-06\n",
      "   -3.19575714e-06]]\n",
      "\n",
      " [[ 3.61958291e-06  3.61958291e-06  3.61958291e-06  3.61958291e-06\n",
      "    3.61958291e-06]\n",
      "  [ 3.61958291e-06  3.61958291e-06  3.61958291e-06  3.61958291e-06\n",
      "    3.61958291e-06]\n",
      "  [ 3.61958291e-06  3.61958291e-06  3.61958291e-06  3.61958291e-06\n",
      "    3.61958291e-06]\n",
      "  [ 3.61958291e-06  3.61958291e-06  3.61958291e-06  3.61958291e-06\n",
      "    3.61958291e-06]\n",
      "  [ 3.61958291e-06  3.61958291e-06  3.61958291e-06  3.61958291e-06\n",
      "    3.61958291e-06]\n",
      "  [ 3.61958291e-06  3.61958291e-06  3.61958291e-06  3.61958291e-06\n",
      "    3.61958291e-06]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 5.52373461e-08  5.52373461e-08  5.52373461e-08  5.52373461e-08\n",
      "    5.52373461e-08]\n",
      "  [ 5.52373461e-08  5.52373461e-08  5.52373461e-08  5.52373461e-08\n",
      "    5.52373461e-08]\n",
      "  [ 5.52373461e-08  5.52373461e-08  5.52373461e-08  5.52373461e-08\n",
      "    5.52373461e-08]\n",
      "  [ 5.52373461e-08  5.52373461e-08  5.52373461e-08  5.52373461e-08\n",
      "    5.52373461e-08]\n",
      "  [ 5.52373461e-08  5.52373461e-08  5.52373461e-08  5.52373461e-08\n",
      "    5.52373461e-08]\n",
      "  [ 5.52373461e-08  5.52373461e-08  5.52373461e-08  5.52373461e-08\n",
      "    5.52373461e-08]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[-3.23730575e-06 -3.23730575e-06 -3.23730575e-06 -3.23730575e-06\n",
      "   -3.23730575e-06]\n",
      "  [-3.23730575e-06 -3.23730575e-06 -3.23730575e-06 -3.23730575e-06\n",
      "   -3.23730575e-06]\n",
      "  [-3.23730575e-06 -3.23730575e-06 -3.23730575e-06 -3.23730575e-06\n",
      "   -3.23730575e-06]\n",
      "  [-3.23730575e-06 -3.23730575e-06 -3.23730575e-06 -3.23730575e-06\n",
      "   -3.23730575e-06]\n",
      "  [-3.23730575e-06 -3.23730575e-06 -3.23730575e-06 -3.23730575e-06\n",
      "   -3.23730575e-06]\n",
      "  [-3.23730575e-06 -3.23730575e-06 -3.23730575e-06 -3.23730575e-06\n",
      "   -3.23730575e-06]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "print(True_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[-9.65693039e-07 -1.22574445e-06 -1.33490619e-06 -1.39466180e-06\n",
      "   -1.43231928e-06]\n",
      "  [-1.51197965e-06 -1.55411422e-06 -1.56851764e-06 -1.57578809e-06\n",
      "   -1.58017256e-06]\n",
      "  [-1.58899178e-06 -1.59342668e-06 -1.59490875e-06 -1.59565049e-06\n",
      "   -1.59609572e-06]\n",
      "  [-1.59698679e-06 -1.59743238e-06 -1.59758104e-06 -1.59765510e-06\n",
      "   -1.59769967e-06]\n",
      "  [-1.59778932e-06 -1.59783429e-06 -1.59784650e-06 -1.59785316e-06\n",
      "   -1.59786260e-06]\n",
      "  [-1.59785982e-06 -1.59788203e-06 -1.59785982e-06 -1.59788203e-06\n",
      "   -1.59786537e-06]]\n",
      "\n",
      " [[ 3.87180482e-06  2.60191602e-06  2.29648932e-06  2.16061850e-06\n",
      "    2.08395030e-06]\n",
      "  [ 1.94069675e-06  1.87377837e-06  1.85213102e-06  1.84142775e-06\n",
      "    1.83504393e-06]\n",
      "  [ 1.82236142e-06  1.81606243e-06  1.81396904e-06  1.81292347e-06\n",
      "    1.81229653e-06]\n",
      "  [ 1.81104342e-06  1.81041764e-06  1.81020891e-06  1.81010400e-06\n",
      "    1.81004210e-06]\n",
      "  [ 1.80991637e-06  1.80985530e-06  1.80983588e-06  1.80982422e-06\n",
      "    1.80981867e-06]\n",
      "  [ 1.80981867e-06  1.80980201e-06  1.80979091e-06  1.80980201e-06\n",
      "    1.80983532e-06]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 3.62018624e-08  3.16960344e-08  3.02955399e-08  2.96116641e-08\n",
      "    2.92062480e-08]\n",
      "  [ 2.84058859e-08  2.80106974e-08  2.78796763e-08  2.78142984e-08\n",
      "    2.77751102e-08]\n",
      "  [ 2.76968309e-08  2.76577454e-08  2.76447169e-08  2.76381943e-08\n",
      "    2.76343085e-08]\n",
      "  [ 2.76265369e-08  2.76225401e-08  2.76213189e-08  2.76206527e-08\n",
      "    2.76202086e-08]\n",
      "  [ 2.76202086e-08  2.76202086e-08  2.76229842e-08  2.76202086e-08\n",
      "    2.76224291e-08]\n",
      "  [ 2.76279802e-08  2.76335313e-08  2.76446335e-08  2.76002245e-08\n",
      "    2.76446336e-08]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[-9.70681491e-07 -1.23691929e-06 -1.34888271e-06 -1.41019003e-06\n",
      "   -1.44882688e-06]\n",
      "  [-1.53055272e-06 -1.57377161e-06 -1.58854380e-06 -1.59599997e-06\n",
      "   -1.60049629e-06]\n",
      "  [-1.60954017e-06 -1.61408789e-06 -1.61560765e-06 -1.61636820e-06\n",
      "   -1.61682481e-06]\n",
      "  [-1.61773842e-06 -1.61819550e-06 -1.61834783e-06 -1.61842365e-06\n",
      "   -1.61846951e-06]\n",
      "  [-1.61856221e-06 -1.61860662e-06 -1.61861994e-06 -1.61862771e-06\n",
      "   -1.61863493e-06]\n",
      "  [-1.61863216e-06 -1.61864326e-06 -1.61864326e-06 -1.61864326e-06\n",
      "   -1.61859885e-06]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "print(Approx_delta)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IFT6135H19_Assignment_1_P1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
