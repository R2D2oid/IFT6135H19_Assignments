{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please run MNIST_download.ipynb to download, pre-process, and store all the necessary data in mnist_dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W7UoFBoCFPKA",
    "outputId": "6af02037-73fe-4983-dd01-e57581659594"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glorot_ReLU_accuracy.png\t  mnist_experiments\r\n",
      "Glorot_ReLU_loss.png\t\t  Normal_ReLU_accuracy.png\r\n",
      "Glorot_Sigmoid_accuracy.png\t  Normal_ReLU_loss.png\r\n",
      "Glorot_Sigmoid_loss.png\t\t  Normal_Sigmoid_accuracy.png\r\n",
      "IFT6135H19_A1_P1_VG.png\t\t  Normal_Sigmoid_loss.png\r\n",
      "IFT6135H19_A1_Pc_no_change.png\t  Plotter.ipynb\r\n",
      "IFT6135H19_A1_Pc.png\t\t  Zero_ReLU_accuracy.png\r\n",
      "IFT6135H19_Assignment_1_P1.ipynb  Zero_ReLU_loss.png\r\n",
      "mnist_dataset\t\t\t  Zero_Sigmoid_accuracy.png\r\n",
      "MNIST_download.ipynb\t\t  Zero_Sigmoid_loss.png\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFgiqz8yYmng"
   },
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/agoose77/numpy-html.git#egg=numpy-html\n",
    "#import numpy_html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "#np.set_printoptions(threshold=5, edgeitems=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1,2,3) Multi-Layer Perceptron in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Helper Function for Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Affine Layer Forward and Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aYky0piO1K9T"
   },
   "outputs": [],
   "source": [
    "def Affine_forward(inp, W, B):\n",
    "  # Params, inp: Input to Layer : (NxD)\n",
    "  #         W: weight of Layer  : (DxM)\n",
    "  #         B  Bias of Layer    : (1xM)\n",
    "  # Output, out = inp*W + B     : (NxM) \n",
    "\n",
    "  out = np.dot(inp,W) + B  # out: (NxD)x(D,M) + (1,M) = (N,M)\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zk1CumTAx7mo"
   },
   "outputs": [],
   "source": [
    "def Affine_backward(inp, W, B, gradient):\n",
    "  # Assume, inp                         : (NxD)\n",
    "  #         W                           : (DxM)\n",
    "  #         B                           : (1XM)\n",
    "  #         gradient                    : (NxM)\n",
    "  #         reg regulazier scaler       : (1x1)  \n",
    "  # Output, Dinp = gradient*Traspose(T) : (NxD)\n",
    "  #         DW = Traspose(inp)*gradient : (NxD)\n",
    "  #         DB = Sum_N gradient         : (1xM) \n",
    "  \n",
    "  Dinp = np.dot(gradient, W.T)                  # DH: (NxM) * (MxD) = (NxD)\n",
    "  DW = np.dot(inp.T, gradient)                # DW  : (DxN) * (NxM) = (DxM)\n",
    "  DB = np.sum(gradient, 0, keepdims=True)     # DB  : Sum_N (N,M)   = (1xM)\n",
    "    \n",
    "  return Dinp, DW, DB\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) ReLU forward and backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "57WN3A_9l6z5"
   },
   "outputs": [],
   "source": [
    "def ReLU(inp):\n",
    "  # Params, inp               : (N,M)\n",
    "  # Output, activ = max(0,inp): (N,M)\n",
    "\n",
    "  activ = np.maximum(0,inp)\n",
    "  \n",
    "  return np.maximum(0,inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x78eTYjMuOCt"
   },
   "outputs": [],
   "source": [
    "def ReLU_backward(inp, gradient):\n",
    "  # Params, inp: Input to ReLU                                             : (N,M)\n",
    "  #         gradient: Backpropogating gradient from the next layer         : (N,M)\n",
    "  # Output: gradient: gradient * Derivative of ReLU (1 if X>0 otherwise 0) : (N,M)\n",
    "  \n",
    "  gradient[inp<=0] = 0\n",
    "  \n",
    "  return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Sigmoid Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DE7no9RdnHdI"
   },
   "outputs": [],
   "source": [
    "def Sigmoid(inp):\n",
    "  # Params, inp                       : (N,M)\n",
    "  # Output, activ = 1 / 1 + exp(-inp) : (N,M)\n",
    "  \n",
    "  activ = 1 / (1 + np.exp(-inp))\n",
    "  \n",
    "  return activ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDKs21WMnsB2"
   },
   "outputs": [],
   "source": [
    "def Sigmoid_backward(inp, gradient):\n",
    "  # Params, inp: Input to ReLU                                                 : (N,M)\n",
    "  #         gradient: Backpropogating gradient from the next layer             : (N,M)\n",
    "  # Output: gradient: gradient * Derivative of Sigmoid (sigmoid * (1-sigmoid)) : (N,M)\n",
    " \n",
    "  s = Sigmoid(inp)  \n",
    "  Dsigmoid = s*(1-s)\n",
    "  \n",
    "  out = gradient * Dsigmoid\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) TanH Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tL9tpCgpN9B"
   },
   "outputs": [],
   "source": [
    "def TanH(inp):\n",
    "  # Params, inp                                         : (N,M)\n",
    "  # Output, activ = (exp(2*inp) - 1) / (exp(2*inp) + 1) : (N,M)\n",
    "  \n",
    "  exp2a = np.exp(2*inp)\n",
    "  activ = (exp2a - 1) / (exp2a + 1)\n",
    "  \n",
    "  return activ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmWtJENpqb5U"
   },
   "outputs": [],
   "source": [
    "def TanH_backward(inp, gradient):\n",
    "  # Params, inp: Input to TanH                                                 : (N,M)\n",
    "  #         gradient: Backpropogating gradient from the next layer             : (N,M)\n",
    "  # Output: gradient: gradient * Derivative of TanH (1-TanH^2))                : (N,M)\n",
    "\n",
    "  tanh = TanH(inp)\n",
    "  Dtanh = 1 - (tanh**2)\n",
    "  \n",
    "  out = gradient * Dtanh\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) SoftMax Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlZAXV3SlMoS"
   },
   "outputs": [],
   "source": [
    "def softmax(inp):\n",
    "  # Params, inp                                        : (N,C)\n",
    "  # Output, out: probs = exp(inp_i) / Sum_j exp(inp_j) : (N,C)\n",
    "    \n",
    "  exp_inp = np.exp(inp)\n",
    "  probs = exp_inp / np.sum(exp_inp, axis=1, keepdims=True)\n",
    "  \n",
    "  return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_stable(inp):\n",
    "  # Params, inp                                        : (N,C)\n",
    "  # Output, out: probs = exp(inp_i) / Sum_j exp(inp_j) : (N,C)\n",
    "  max_inp = np.max(inp,1, keepdims=True)\n",
    "  exp_inp = np.exp(inp - max_inp)\n",
    "  probs = exp_inp / np.sum(exp_inp, axis=1, keepdims=True)\n",
    "  \n",
    "  return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yBV9l8ifh0Ja"
   },
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_backward(probs, GT):\n",
    "  # Params, probs: Network output probabilities                                                : (N,C)\n",
    "  #         GT:  each value belongs to one of C classes                                        : (N,)\n",
    "  # Output, Gradient of Pre-SoftMax activation with respect to output (dout = -(e(y) - probs)) : (N,C)          \n",
    "\n",
    "  dout = probs\n",
    "  dout[range(GT.shape[0]),GT] -= 1\n",
    "  dout /= GT.shape[0] # take mean of the gradient of the batch\n",
    "\n",
    "  return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9I66WVX3g8jc"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(probs, GT):\n",
    "  # Params, probs: (N,C) where sum_c = 1\n",
    "  #         GT   : (N,) \n",
    "  # Output, loss : scalar\n",
    "  \n",
    "  # compute log probability for true value of GT for each example\n",
    "  logprobs = -np.log(probs[range(GT.shape[0]),GT])\n",
    "\n",
    "  loss = np.sum(logprobs)\n",
    "  loss /= GT.shape[0]\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Function to shuffle training data after epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fAwhNjj-apAv"
   },
   "outputs": [],
   "source": [
    "def shuffle_data(X,Y):\n",
    "   \n",
    "  s = np.arange(X.shape[0])\n",
    "  np.random.shuffle(s)\n",
    "\n",
    "  return X[s], Y[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Saving and Loading python objects as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S8WlMQ7lUufq"
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, path, name):\n",
    "    with open(path + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3NGOlFkVJAL"
   },
   "outputs": [],
   "source": [
    "def load_obj(path, name):\n",
    "    with open(path + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Neural Network Class Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that NN class has a function named initialization which allows us to initialize NN with any initialization method (glorot, normal, or zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fs9lThS8rZVH"
   },
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "\n",
    "  \n",
    "  def __init__(self,\n",
    "               input_dims=784, \n",
    "               output_dims=10, \n",
    "               hidden_dims=(1024,2048), \n",
    "               n_hidden=2, \n",
    "               regularization_coefficient=0,\n",
    "               init_method='zeros',           # 'zeros', 'normal', or 'glorot'\n",
    "               nonlinearity = 'relu',         # 'relu', 'tanh', or 'sigmoid'\n",
    "               LearningRate = 0.001):\n",
    "    \n",
    "    dims = (input_dims,) + hidden_dims + (output_dims,)    \n",
    "    self.n_hidden = n_hidden\n",
    "     \n",
    "    self.LR = LearningRate\n",
    "    self.reg_coeff = regularization_coefficient\n",
    "    \n",
    "    self.params = {}    # dictionary of parameters of NN\n",
    "    self.cache = {}     # dictionary of forward model cache\n",
    "    self.gradient = {}  # dictionary of backward model cache (gradient)\n",
    "\n",
    "    self.activ = nonlinearity\n",
    "    self.initialize_weights(n_hidden, dims, init_method)\n",
    "         \n",
    "        \n",
    "  def initialize_weights(self,n_hidden, dims, init_method):\n",
    "    \n",
    "    # W = weight parameters\n",
    "    # B = Biases\n",
    "    \n",
    "    for i in range(1,n_hidden+2):\n",
    "      \n",
    "      self.params['B'+str(i)] = np.zeros((1,dims[i]))\n",
    "      \n",
    "      if init_method == 'zeros':\n",
    "        self.params['W'+str(i)] = np.zeros((dims[i-1],dims[i]))\n",
    "      elif init_method == 'normal':\n",
    "        self.params['W'+str(i)] = np.random.normal(0.0, 1.0, (dims[i-1],dims[i]))        \n",
    "      elif init_method == 'glorot':\n",
    "        dl = np.sqrt(6/(dims[i-1]+dims[i]))\n",
    "        self.params['W'+str(i)] = np.random.uniform(-dl, dl, (dims[i-1],dims[i]))\n",
    "      else:\n",
    "        raise Exception('Weight Intialization Method should be one of the following: zeros, glorot, or normal') \n",
    "    \n",
    "    # DA = gradient of W\n",
    "    # DB = gradient pf B\n",
    "    for i in range(1,n_hidden+2):\n",
    "\n",
    "      self.gradient['DB'+str(i)] = np.zeros((1,dims[i]))     \n",
    "      self.gradient['DW'+str(i)] = np.zeros((dims[i-1],dims[i]))\n",
    "\n",
    "        \n",
    "  def activation(self,input):\n",
    "    \n",
    "    if self.activ == 'relu':\n",
    "      out = ReLU(input)\n",
    "    elif self.activ == 'tanh':\n",
    "      out = TanH(input)\n",
    "    elif self.activ == 'sigmoid':\n",
    "      out = Sigmoid(input)\n",
    "    else:\n",
    "      raise Exception('NonLinearity should be one of the following: relu, tanh, or sigmoid')\n",
    "        \n",
    "    return out\n",
    "        \n",
    "\n",
    "  def activation_backward(self,input,gradient):\n",
    "    \n",
    "    if self.activ == 'relu':\n",
    "      out = ReLU_backward(input, gradient)\n",
    "    elif self.activ == 'tanh':\n",
    "      out = TanH_backward(input, gradient)\n",
    "    elif self.activ == 'sigmoid':\n",
    "      out = Sigmoid_backward(input, gradient)\n",
    "    else:\n",
    "      raise Exception('NonLinearity should be one of the following: relu, tanh, or sigmoid')\n",
    "        \n",
    "    return out\n",
    "\n",
    "  \n",
    "  def forward(self,input):    \n",
    "    \n",
    "    # W = weight parameters\n",
    "    # B = Biases\n",
    "    # A = pre-activation (affine transformed input)\n",
    "    # H = post-activation (Hidden layer output)\n",
    "\n",
    "    self.cache['input'] = input\n",
    "    self.cache['A1'] = Affine_forward(self.cache['input'], self.params['W1'], self.params['B1']) # A1 = X*W1 + B1\n",
    "\n",
    "    for i in range(1,self.n_hidden+1):\n",
    "      self.cache['H'+str(i)] = self.activation(self.cache['A'+str(i)]) # Hi = active(Ai), ex: H1=active(A1)\n",
    "      self.cache['A'+str(i+1)] = Affine_forward(self.cache['H'+str(i)], self.params['W'+str(i+1)], self.params['B'+str(i+1)]) # A_(i+1) = H_(i)*W_(i+1) + B_(i+1), ex: A2 = H1*W2 + B2\n",
    "\n",
    "    self.cache['out'] = softmax_stable(self.cache['A'+str(self.n_hidden+1)]) # out = softmax(A3)\n",
    "      \n",
    "    return self.cache['out']\n",
    "\n",
    "  \n",
    "  def loss(self,prediction,labels):\n",
    "\n",
    "    data_loss = cross_entropy_loss(prediction, labels)\n",
    "    \n",
    "    reg_loss = 0\n",
    "    \n",
    "    # if regularization parameter if greater than 0 than calculate L2 value of all weight layer to calculate reg_loss\n",
    "    if self.reg_coeff>0:\n",
    "      reg_loss = sum(np.sum(self.params['W'+str(i)]**2) for i in range(1,self.n_hidden+2))\n",
    "\n",
    "    loss = data_loss + (self.reg_coeff * reg_loss)\n",
    "    \n",
    "    return loss\n",
    "  \n",
    "  \n",
    "  def backward(self, labels):\n",
    "    \n",
    "    # DA is a dummy variable to store gradient of pre-activation\n",
    "    # DH is a dummy variable to store gradient of post-pactivation\n",
    "\n",
    "    DA = softmax_cross_entropy_backward(self.cache['out'], labels) \n",
    "    \n",
    "    for i in range(self.n_hidden+1, 1, -1):\n",
    "      DH, self.gradient['DW'+str(i)], self.gradient['DB'+str(i)] =  Affine_backward(self.cache['H'+str(i-1)], self.params['W'+str(i)], self.params['B'+str(i)], DA) # DH2, DW3, DB3 = backward(H2, W3, B3)\n",
    "      DA = self.activation_backward(self.cache['A'+str(i-1)], DH) #DA2 = backward(A2, DH2)\n",
    " \n",
    "    _, self.gradient['DW1'], self.gradient['DB1'] = Affine_backward(self.cache['input'], self.params['W1'], self.params['B1'], DA) # Dinp, DW1, DB1 = backward(input, W1, B1, DA1)\n",
    "\n",
    "     \n",
    "  def update(self):\n",
    "    \n",
    "    # Update NN parameters\n",
    "    \n",
    "    for i in range(1,self.n_hidden+2):\n",
    "\n",
    "      self.params['B'+str(i)] -= self.LR*self.gradient['DB'+str(i)] \n",
    "      \n",
    "      if self.reg_coeff>0:\n",
    "        self.gradient['DW'+str(i)] += self.reg_coeff * self.params['W'+str(i)]\n",
    "\n",
    "      self.params['W'+str(i)] -= self.LR*self.gradient['DW'+str(i)]\n",
    "  \n",
    "  \n",
    "  def zero_gradient(self):\n",
    "    \n",
    "    # make gradients zero\n",
    "    # useful to do it at the starting of epocj\n",
    "\n",
    "    for i in range(1,n_hidden+2):\n",
    "\n",
    "      self.gradient['DB'+str(i)] = np.zeros(self.gradient['DB'+str(i)].shape)     \n",
    "      self.gradient['DW'+str(i)] = np.zeros(self.gradient['DW'+str(i)].shape)\n",
    "      \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function to train model for one epoch (pass through all training images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcCJ5vNQbQse"
   },
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, batch_size):\n",
    "    \n",
    "  samples = x_train.shape[0]\n",
    "\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for i in range(0, samples, batch_size):\n",
    "      \n",
    "    data, label = x_train[i:i+batch_size], y_train[i:i+batch_size] # get batch of data and label\n",
    "      \n",
    "    pred_probs = model.forward(data) # forward pass of the data\n",
    "      \n",
    "    batch_loss = model.loss(pred_probs, label) # calculate loss with respect to label\n",
    "      \n",
    "    epoch_loss += batch_loss # increment epoch loss\n",
    "      \n",
    "    model.backward(label) # backward pass with respect to label\n",
    "      \n",
    "    model.update() # update model parameters\n",
    "      \n",
    "    model.zero_gradient() # make gradient zero\n",
    "      \n",
    "  epoch_loss = (epoch_loss * batch_size) / (samples) # normalize epoch loss with respect to batch size and total samples\n",
    "    \n",
    "  return epoch_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function to get prediction for all test data and calculate loss and accuracy for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SeptjN7t9j6V"
   },
   "outputs": [],
   "source": [
    "def test(model, x, y):\n",
    "\n",
    "  model.zero_gradient() # make model gradient to zero\n",
    "  \n",
    "  samples = x.shape[0] # calculate total data samples\n",
    "  \n",
    "  epoch_loss = 0       # to store loss for the wholte dataset\n",
    "  true_prediction = 0  # to store total true prediction\n",
    "  \n",
    "  for i in range(0, samples):\n",
    "      \n",
    "    data, label = x[i:i+1], y[i:i+1] # get a sample of data\n",
    "      \n",
    "    pred_probs = model.forward(data) # forward pass through model\n",
    "    \n",
    "    pred_labels = np.argmax(pred_probs) # find the predicted label\n",
    "    \n",
    "    batch_loss = model.loss(pred_probs, label) # calculate loss\n",
    "\n",
    "    true_prediction += np.count_nonzero(pred_labels-label == 0) # check if prediction and true label are same or not\n",
    "    \n",
    "    epoch_loss += batch_loss # increment epoch loss\n",
    "            \n",
    "  epoch_loss = epoch_loss / samples # normalize loss with batch_size and samples\n",
    "  \n",
    "  accuracy = true_prediction / samples # calculate accuracy\n",
    "  \n",
    "  return epoch_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QG9ugMXk1p9y"
   },
   "outputs": [],
   "source": [
    "# experiment parameters\n",
    "total_epoch = 10\n",
    "\n",
    "data_path = 'mnist_dataset/mnist.npy'\n",
    "model_path = 'mnist_experiments/Exp_Best_performing_architecture/'\n",
    "\n",
    "os.makedirs(model_path,exist_ok=True)\n",
    "\n",
    "input_dims=784 \n",
    "output_dims=10 \n",
    "hidden_dims=(512,384) \n",
    "n_hidden=2 \n",
    "regularization_coefficient=0\n",
    "init_method ='glorot'          # 'zeros', 'normal', or 'glorot'\n",
    "nonlinearity = 'relu'         # 'relu', 'tanh', or 'sigmoid'\n",
    "LearningRate = 0.1\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read MNIST data and pre-process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rK2y5aiy4_ff"
   },
   "outputs": [],
   "source": [
    "# data-preparation\n",
    "(tr, va, te) = np.load(data_path)\n",
    "    \n",
    "x_train = tr[0]\n",
    "y_train = tr[1]\n",
    "x_valid = va[0]\n",
    "y_valid = va[1]\n",
    "x_test  = te[0]\n",
    "y_test  = te[1]\n",
    "    \n",
    "data_mean = x_train.mean()\n",
    "data_std  = x_train.std()\n",
    "    \n",
    "x_train = (x_train - data_mean) / data_std \n",
    "x_test  = (x_test  - data_mean) / data_std \n",
    "x_valid = (x_valid - data_mean) / data_std \n",
    "\n",
    "del tr, va, te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D8OJ0Ycs6tyd",
    "outputId": "f9f4c680-ed0e-4639-d0cb-9f2029c28e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Model Parameters:  602762\n"
     ]
    }
   ],
   "source": [
    "model = NN(input_dims, output_dims, hidden_dims, n_hidden, regularization_coefficient, init_method, nonlinearity, LearningRate)\n",
    "\n",
    "model_params = sum(value.size for _, value in model.params.items())\n",
    "\n",
    "print('Total Model Parameters: ', model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network for total_epochs and print and store training and validation loss and accuracy after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "3X4YHiBqBIOA",
    "outputId": "a6c1709d-a301-4bc3-c6c9-3d8ee316d2e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Training Done Epoch 1\n",
      "===> Training   Epoch 1: Loss - 0.1081, Accuracy - 0.9660\n",
      "===> Validation Epoch 1: Loss - 0.1345, Accuracy - 0.9606\n",
      "===> Model Saved Epoch 1\n",
      "===> Training Done Epoch 2\n",
      "===> Training   Epoch 2: Loss - 0.0757, Accuracy - 0.9755\n",
      "===> Validation Epoch 2: Loss - 0.1160, Accuracy - 0.9672\n",
      "===> Model Saved Epoch 2\n",
      "===> Training Done Epoch 3\n",
      "===> Training   Epoch 3: Loss - 0.0373, Accuracy - 0.9881\n",
      "===> Validation Epoch 3: Loss - 0.0959, Accuracy - 0.9735\n",
      "===> Model Saved Epoch 3\n",
      "===> Training Done Epoch 4\n",
      "===> Training   Epoch 4: Loss - 0.0406, Accuracy - 0.9864\n",
      "===> Validation Epoch 4: Loss - 0.1118, Accuracy - 0.9720\n",
      "===> Model Saved Epoch 4\n",
      "===> Training Done Epoch 5\n",
      "===> Training   Epoch 5: Loss - 0.0355, Accuracy - 0.9886\n",
      "===> Validation Epoch 5: Loss - 0.1153, Accuracy - 0.9736\n",
      "===> Model Saved Epoch 5\n",
      "===> Training Done Epoch 6\n",
      "===> Training   Epoch 6: Loss - 0.0194, Accuracy - 0.9941\n",
      "===> Validation Epoch 6: Loss - 0.1013, Accuracy - 0.9775\n",
      "===> Model Saved Epoch 6\n",
      "===> Training Done Epoch 7\n",
      "===> Training   Epoch 7: Loss - 0.0151, Accuracy - 0.9951\n",
      "===> Validation Epoch 7: Loss - 0.1019, Accuracy - 0.9772\n",
      "===> Model Saved Epoch 7\n",
      "===> Training Done Epoch 8\n",
      "===> Training   Epoch 8: Loss - 0.0181, Accuracy - 0.9940\n",
      "===> Validation Epoch 8: Loss - 0.1144, Accuracy - 0.9769\n",
      "===> Model Saved Epoch 8\n",
      "===> Training Done Epoch 9\n",
      "===> Training   Epoch 9: Loss - 0.0146, Accuracy - 0.9954\n",
      "===> Validation Epoch 9: Loss - 0.1118, Accuracy - 0.9784\n",
      "===> Model Saved Epoch 9\n",
      "===> Training Done Epoch 10\n",
      "===> Training   Epoch 10: Loss - 0.0112, Accuracy - 0.9962\n",
      "===> Validation Epoch 10: Loss - 0.0981, Accuracy - 0.9822\n",
      "===> Model Saved Epoch 10\n",
      "===> Testing Accuracy at the end of training: 0.9799\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "  \n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for i in range(1,total_epoch+1):\n",
    "  \n",
    "  loss = train(model, x_train, y_train, batch_size)\n",
    "  \n",
    "  print(\"===> Training Done Epoch {}\".format(i))\n",
    "  \n",
    "  loss, accuracy = test(model, x_train, y_train)\n",
    "  \n",
    "  train_loss = np.concatenate((train_loss, [loss]))\n",
    "  train_accuracy = np.concatenate((train_accuracy, [accuracy]))\n",
    "\n",
    "  print(\"===> Training   Epoch {}: Loss - {:.4f}, Accuracy - {:.4f}\".format(i, train_loss[-1], train_accuracy[-1]))\n",
    "\n",
    "  loss, accuracy = test(model, x_valid, y_valid)\n",
    "  \n",
    "  valid_loss = np.concatenate((valid_loss, [loss]))\n",
    "  valid_accuracy = np.concatenate((valid_accuracy, [accuracy]))\n",
    "\n",
    "  print(\"===> Validation Epoch {}: Loss - {:.4f}, Accuracy - {:.4f}\".format(i, valid_loss[-1], valid_accuracy[-1]))\n",
    "  \n",
    "  save_obj(model.params, model_path, 'model-{:02d}'.format(i))\n",
    "  \n",
    "  print(\"===> Model Saved Epoch {}\".format(i))\n",
    "\n",
    "  shuffle_data(x_train, y_train)\n",
    "\n",
    "  \n",
    "save_obj(train_loss, model_path, 'train-loss')\n",
    "save_obj(train_accuracy, model_path, 'train-accuracy')\n",
    "save_obj(valid_loss, model_path, 'valid-loss')\n",
    "save_obj(valid_accuracy, model_path, 'valid-accuracy')\n",
    "\n",
    "\n",
    "_, test_accuracy = test(model, x_test, y_test)\n",
    "\n",
    "print(\"===> Testing Accuracy at the end of training: {:.4f}\".format(test_accuracy) )\n",
    "\n",
    "save_obj([test_accuracy], model_path, 'test-accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4) Validate Gradient using finite difference at the end of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize new model for gradient validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims=784 \n",
    "output_dims=10 \n",
    "hidden_dims=(512,384) \n",
    "n_hidden=2 \n",
    "regularization_coefficient=0\n",
    "init_method ='glorot'          # 'zeros', 'normal', or 'glorot'\n",
    "nonlinearity = 'relu'         # 'relu', 'tanh', or 'sigmoid'\n",
    "LearningRate = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Model Parameters:  602762\n"
     ]
    }
   ],
   "source": [
    "model = NN(input_dims, output_dims, hidden_dims, n_hidden, regularization_coefficient, init_method, nonlinearity, LearningRate)\n",
    "\n",
    "model_params = sum(value.size for _, value in model.params.items())\n",
    "\n",
    "print('Total Model Parameters: ', model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a single training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = x_train[11:12], y_train[11:12]\n",
    "      \n",
    "pred_probs = model.forward(data)\n",
    "                  \n",
    "model.backward(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store original params W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_W2 = np.copy(model.params['W2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Maximum valur of i, j, and K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_i = 10 # first 10-elements of second layer weight\n",
    "max_j = 6 # j = {0,1,2,3,4,5}\n",
    "max_k = 5 # k = {1,2,3,4,5}\n",
    "\n",
    "Approx_delta = np.zeros((max_i, max_j, max_k)) #(10,6,5)\n",
    "\n",
    "True_delta = model.gradient['DW2'][0][0:max_i]\n",
    "True_delta = np.repeat(True_delta[:,np.newaxis],max_j,axis=-1)\n",
    "True_delta = np.repeat(True_delta[:,:,np.newaxis],max_k,axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.03857505  0.          0.          0.00343996\n",
      " -0.0933025   0.          0.05017874  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(model.gradient['DW2'][0][0:max_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### approximate gradient with finite difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,max_i):\n",
    "    for j in range(0,max_j):\n",
    "        for k in range(0,max_k):\n",
    "            N = (k+1)*(10**j)\n",
    "            epsi = 1 / N\n",
    "            ## positive\n",
    "            model.params['W2'][0][i] += epsi\n",
    "            pred_probs = model.forward(data)\n",
    "            loss = model.loss(pred_probs, label)\n",
    "            pos = loss\n",
    "            model.params['W2'] = np.copy(orig_W2)\n",
    "            ## negative\n",
    "            model.params['W2'][0][i] -= epsi\n",
    "            pred_probs = model.forward(data)\n",
    "            loss = model.loss(pred_probs, label)\n",
    "            neg = loss\n",
    "            model.params['W2'] = np.copy(orig_W2)\n",
    "            ## Approximate Delta\n",
    "            Approx_delta[i][j][k] = (pos-neg) / (2*epsi)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate maximum difference between finite difference and true difference for difference values of N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.zeros((max_j*max_k,))\n",
    "max_diff = np.zeros((max_j*max_k,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.e+00 2.e+00 3.e+00 4.e+00 5.e+00 1.e+01 2.e+01 3.e+01 4.e+01 5.e+01\n",
      " 1.e+02 2.e+02 3.e+02 4.e+02 5.e+02 1.e+03 2.e+03 3.e+03 4.e+03 5.e+03\n",
      " 1.e+04 2.e+04 3.e+04 4.e+04 5.e+04 1.e+05 2.e+05 3.e+05 4.e+05 5.e+05]\n",
      "[2.54267437e-02 2.51283901e-02 2.45168859e-02 2.38258512e-02\n",
      " 2.31028062e-02 1.93428436e-02 1.23916901e-02 9.87051985e-03\n",
      " 7.35454956e-03 4.84065908e-03 2.30767366e-09 5.76924779e-10\n",
      " 2.56403392e-10 1.44226457e-10 9.23901441e-11 2.32787609e-11\n",
      " 5.84825938e-12 3.07270182e-12 1.40736728e-12 1.18532267e-12\n",
      " 1.43297180e-12 2.74418769e-12 4.76364087e-12 7.43975714e-12\n",
      " 3.65341785e-12 2.58669197e-11 8.35804299e-12 4.71531087e-11\n",
      " 7.49714245e-11 8.20814042e-11]\n"
     ]
    }
   ],
   "source": [
    "#print(Approx_delta[9][0][0])\n",
    "#print(True_delta[9][0][0])\n",
    "i = 0\n",
    "for j in range(max_j):\n",
    "    for k in range(max_k):\n",
    "        N[i] = (k+1)*(10**j)\n",
    "        max_diff[i] = np.max(np.abs(True_delta[:,j,k] - Approx_delta[:,j,k]))\n",
    "        i = i+1\n",
    "    \n",
    "\n",
    "print(N)\n",
    "print(max_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot this value as a function of N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(0)\n",
    "plt.semilogx(N, max_diff)\n",
    "#plt.title('Maximum Difference between the true gradient and the finite gradient as a function of N')\n",
    "plt.grid(True)\n",
    "plt.xlabel('N=(k)*(10^j) where k={1,...,5}, j={0,...,5}')\n",
    "plt.ylabel('max_{1<=i<=10} |Delta_i^N - DL/DT_i|')\n",
    "#plt.show()\n",
    "plt.savefig('IFT6135H19_A1_P1_VG.png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.03857505  0.03857505  0.03857505  0.03857505  0.03857505]\n",
      "  [ 0.03857505  0.03857505  0.03857505  0.03857505  0.03857505]\n",
      "  [ 0.03857505  0.03857505  0.03857505  0.03857505  0.03857505]\n",
      "  [ 0.03857505  0.03857505  0.03857505  0.03857505  0.03857505]\n",
      "  [ 0.03857505  0.03857505  0.03857505  0.03857505  0.03857505]\n",
      "  [ 0.03857505  0.03857505  0.03857505  0.03857505  0.03857505]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.00343996  0.00343996  0.00343996  0.00343996  0.00343996]\n",
      "  [ 0.00343996  0.00343996  0.00343996  0.00343996  0.00343996]\n",
      "  [ 0.00343996  0.00343996  0.00343996  0.00343996  0.00343996]\n",
      "  [ 0.00343996  0.00343996  0.00343996  0.00343996  0.00343996]\n",
      "  [ 0.00343996  0.00343996  0.00343996  0.00343996  0.00343996]\n",
      "  [ 0.00343996  0.00343996  0.00343996  0.00343996  0.00343996]]\n",
      "\n",
      " [[-0.0933025  -0.0933025  -0.0933025  -0.0933025  -0.0933025 ]\n",
      "  [-0.0933025  -0.0933025  -0.0933025  -0.0933025  -0.0933025 ]\n",
      "  [-0.0933025  -0.0933025  -0.0933025  -0.0933025  -0.0933025 ]\n",
      "  [-0.0933025  -0.0933025  -0.0933025  -0.0933025  -0.0933025 ]\n",
      "  [-0.0933025  -0.0933025  -0.0933025  -0.0933025  -0.0933025 ]\n",
      "  [-0.0933025  -0.0933025  -0.0933025  -0.0933025  -0.0933025 ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.05017874  0.05017874  0.05017874  0.05017874  0.05017874]\n",
      "  [ 0.05017874  0.05017874  0.05017874  0.05017874  0.05017874]\n",
      "  [ 0.05017874  0.05017874  0.05017874  0.05017874  0.05017874]\n",
      "  [ 0.05017874  0.05017874  0.05017874  0.05017874  0.05017874]\n",
      "  [ 0.05017874  0.05017874  0.05017874  0.05017874  0.05017874]\n",
      "  [ 0.05017874  0.05017874  0.05017874  0.05017874  0.05017874]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(True_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.00101029  0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.01775617  0.01719216  0.01683691  0.01653378  0.01625148]\n",
      "  [ 0.01493366  0.01239169  0.00987052  0.00735455  0.00484066]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.02356996  0.02675171  0.03017828  0.03366599  0.03717815]\n",
      "  [ 0.03857507  0.03857505  0.03857505  0.03857505  0.03857505]\n",
      "  [ 0.03857505  0.03857505  0.03857505  0.03857505  0.03857505]\n",
      "  [ 0.03857505  0.03857505  0.03857505  0.03857505  0.03857505]\n",
      "  [ 0.03857505  0.03857505  0.03857505  0.03857505  0.03857505]\n",
      "  [ 0.03857505  0.03857505  0.03857505  0.03857505  0.03857505]]\n",
      "\n",
      " [[-0.02495775 -0.00556121  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.02542674 -0.02512839 -0.02451689 -0.02382585 -0.02310281]\n",
      "  [-0.01934284 -0.01167759 -0.00397993  0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.00255683  0.00256362  0.00275417  0.00299087  0.00324604]\n",
      "  [ 0.0034399   0.00343995  0.00343995  0.00343996  0.00343996]\n",
      "  [ 0.00343996  0.00343996  0.00343996  0.00343996  0.00343996]\n",
      "  [ 0.00343996  0.00343996  0.00343996  0.00343996  0.00343996]\n",
      "  [ 0.00343996  0.00343996  0.00343996  0.00343996  0.00343996]\n",
      "  [ 0.00343996  0.00343996  0.00343996  0.00343996  0.00343996]]\n",
      "\n",
      " [[-0.08189565 -0.09330155 -0.09330208 -0.09330226 -0.09330235]\n",
      "  [-0.09330246 -0.09330249 -0.0933025  -0.0933025  -0.0933025 ]\n",
      "  [-0.0933025  -0.0933025  -0.0933025  -0.0933025  -0.0933025 ]\n",
      "  [-0.0933025  -0.0933025  -0.0933025  -0.0933025  -0.0933025 ]\n",
      "  [-0.0933025  -0.0933025  -0.0933025  -0.0933025  -0.0933025 ]\n",
      "  [-0.0933025  -0.0933025  -0.0933025  -0.0933025  -0.0933025 ]]\n",
      "\n",
      " [[ 0.00015185 -0.00013557 -0.00022211 -0.00025783 -0.00027312]\n",
      "  [-0.00025731 -0.00013314  0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.03455674  0.0429613   0.05018131  0.05018018  0.05017966]\n",
      "  [ 0.05017897  0.0501788   0.05017877  0.05017876  0.05017875]\n",
      "  [ 0.05017874  0.05017874  0.05017874  0.05017874  0.05017874]\n",
      "  [ 0.05017874  0.05017874  0.05017874  0.05017874  0.05017874]\n",
      "  [ 0.05017874  0.05017874  0.05017874  0.05017874  0.05017874]\n",
      "  [ 0.05017874  0.05017874  0.05017874  0.05017874  0.05017874]]\n",
      "\n",
      " [[-0.01483205 -0.01117021 -0.00724358 -0.00325074  0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(Approx_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.01029147e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "\n",
      " [[1.77561738e-02 1.71921616e-02 1.68369129e-02 1.65337787e-02\n",
      "   1.62514769e-02]\n",
      "  [1.49336637e-02 1.23916901e-02 9.87051985e-03 7.35454956e-03\n",
      "   4.84065908e-03]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "\n",
      " [[1.50050885e-02 1.18233349e-02 8.39676913e-03 4.90905615e-03\n",
      "   1.39689478e-03]\n",
      "  [2.58659653e-08 6.46648434e-09 2.87399692e-09 1.61662383e-09\n",
      "   1.03463826e-09]\n",
      "  [2.58670078e-10 6.46697071e-11 2.87095833e-11 1.61973698e-11\n",
      "   1.03353923e-11]\n",
      "  [2.45280879e-12 7.87474252e-13 2.32362740e-13 5.44793377e-13\n",
      "   7.87474252e-13]\n",
      "  [1.43297180e-12 1.43297180e-12 4.76364087e-12 3.65341785e-12\n",
      "   3.65341785e-12]\n",
      "  [7.44881240e-12 3.65341785e-12 1.85510426e-11 3.65341785e-12\n",
      "   5.18577334e-11]]\n",
      "\n",
      " [[2.49577485e-02 5.56121028e-03 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "\n",
      " [[2.54267437e-02 2.51283901e-02 2.45168859e-02 2.38258512e-02\n",
      "   2.31028062e-02]\n",
      "  [1.93428436e-02 1.16775887e-02 3.97992889e-03 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "\n",
      " [[8.83130169e-04 8.76344743e-04 6.85788908e-04 4.49095499e-04\n",
      "   1.93916277e-04]\n",
      "  [6.01785558e-08 1.50447058e-08 6.68653716e-09 3.76118720e-09\n",
      "   2.40715587e-09]\n",
      "  [6.01783351e-10 1.50444385e-10 6.69223075e-11 3.76235215e-11\n",
      "   2.41232095e-11]\n",
      "  [5.91555190e-12 1.69670441e-12 1.14159290e-12 8.08525989e-13\n",
      "   5.86481384e-13]\n",
      "  [5.23741640e-13 2.74418769e-12 5.23741640e-13 5.23741640e-13\n",
      "   2.74418726e-12]\n",
      "  [2.74418726e-12 8.35804299e-12 4.71531087e-11 7.49714245e-11\n",
      "   3.05625030e-11]]\n",
      "\n",
      " [[1.14068487e-02 9.45989376e-07 4.20617618e-07 2.36632440e-07\n",
      "   1.51455140e-07]\n",
      "  [3.78672439e-08 9.46702620e-09 4.20758776e-09 2.36677027e-09\n",
      "   1.51472850e-09]\n",
      "  [3.78698337e-10 9.46810824e-11 4.20898177e-11 2.36490133e-11\n",
      "   1.51558072e-11]\n",
      "  [3.88704346e-12 5.56374391e-13 5.56374391e-13 7.78418996e-13\n",
      "   3.31804029e-13]\n",
      "  [7.78418996e-13 1.44202705e-12 1.44202705e-12 7.43975714e-12\n",
      "   1.88865590e-12]\n",
      "  [2.58669197e-11 7.43977102e-12 1.47647033e-11 1.47646895e-11\n",
      "   2.58669336e-11]]\n",
      "\n",
      " [[1.51847600e-04 1.35574928e-04 2.22114927e-04 2.57830321e-04\n",
      "   2.73119349e-04]\n",
      "  [2.57308280e-04 1.33140790e-04 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]]\n",
      "\n",
      " [[1.56220014e-02 7.21743988e-03 2.56396599e-06 1.44226111e-06\n",
      "   9.23056066e-07]\n",
      "  [2.30767003e-07 5.76919397e-08 2.56408779e-08 1.44229957e-08\n",
      "   9.23071913e-09]\n",
      "  [2.30767366e-09 5.76924779e-10 2.56403392e-10 1.44226457e-10\n",
      "   9.23901441e-11]\n",
      "  [2.32787609e-11 5.84825938e-12 3.07270182e-12 1.40736728e-12\n",
      "   1.18532267e-12]\n",
      "  [7.50996487e-14 2.14534640e-12 7.50996487e-14 7.50996487e-14\n",
      "   1.18531573e-12]\n",
      "  [1.78386611e-11 4.36579939e-12 6.73643780e-12 2.65702599e-11\n",
      "   8.20814042e-11]]\n",
      "\n",
      " [[1.48320458e-02 1.11702084e-02 7.24358095e-03 3.25073502e-03\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]\n",
      "  [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "   0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.abs(True_delta - Approx_delta))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IFT6135H19_Assignment_1_P1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
