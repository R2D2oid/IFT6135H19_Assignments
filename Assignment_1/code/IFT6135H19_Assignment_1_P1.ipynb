{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please run MNIST_download.ipynb to download, pre-process, and store all the necessary data in mnist_dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W7UoFBoCFPKA",
    "outputId": "6af02037-73fe-4983-dd01-e57581659594"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IFT6135H19_A1_Pc.png\t\t  mnist_dataset\t\tmnist_experiments\r\n",
      "IFT6135H19_Assignment_1_P1.ipynb  MNIST_download.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFgiqz8yYmng"
   },
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/agoose77/numpy-html.git#egg=numpy-html\n",
    "#import numpy_html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "#np.set_printoptions(threshold=5, edgeitems=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1,2,3) Multi-Layer Perceptron in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Helper Function for Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Affine Layer Forward and Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aYky0piO1K9T"
   },
   "outputs": [],
   "source": [
    "def Affine_forward(inp, W, B):\n",
    "  # Params, inp: Input to Layer : (NxD)\n",
    "  #         W: weight of Layer  : (DxM)\n",
    "  #         B  Bias of Layer    : (1xM)\n",
    "  # Output, out = inp*W + B     : (NxM) \n",
    "\n",
    "  out = np.dot(inp,W) + B  # out: (NxD)x(D,M) + (1,M) = (N,M)\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zk1CumTAx7mo"
   },
   "outputs": [],
   "source": [
    "def Affine_backward(inp, W, B, gradient):\n",
    "  # Assume, inp                         : (NxD)\n",
    "  #         W                           : (DxM)\n",
    "  #         B                           : (1XM)\n",
    "  #         gradient                    : (NxM)\n",
    "  #         reg regulazier scaler       : (1x1)  \n",
    "  # Output, Dinp = gradient*Traspose(T) : (NxD)\n",
    "  #         DW = Traspose(inp)*gradient : (NxD)\n",
    "  #         DB = Sum_N gradient         : (1xM) \n",
    "  \n",
    "  Dinp = np.dot(gradient, W.T)                  # DH: (NxM) * (MxD) = (NxD)\n",
    "  DW = np.dot(inp.T, gradient)                # DW  : (DxN) * (NxM) = (DxM)\n",
    "  DB = np.sum(gradient, 0, keepdims=True)     # DB  : Sum_N (N,M)   = (1xM)\n",
    "    \n",
    "  return Dinp, DW, DB\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) ReLU forward and backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "57WN3A_9l6z5"
   },
   "outputs": [],
   "source": [
    "def ReLU(inp):\n",
    "  # Params, inp               : (N,M)\n",
    "  # Output, activ = max(0,inp): (N,M)\n",
    "\n",
    "  activ = np.maximum(0,inp)\n",
    "  \n",
    "  return np.maximum(0,inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x78eTYjMuOCt"
   },
   "outputs": [],
   "source": [
    "def ReLU_backward(inp, gradient):\n",
    "  # Params, inp: Input to ReLU                                             : (N,M)\n",
    "  #         gradient: Backpropogating gradient from the next layer         : (N,M)\n",
    "  # Output: gradient: gradient * Derivative of ReLU (1 if X>0 otherwise 0) : (N,M)\n",
    "  \n",
    "  gradient[inp<=0] = 0\n",
    "  \n",
    "  return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Sigmoid Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DE7no9RdnHdI"
   },
   "outputs": [],
   "source": [
    "def Sigmoid(inp):\n",
    "  # Params, inp                       : (N,M)\n",
    "  # Output, activ = 1 / 1 + exp(-inp) : (N,M)\n",
    "  \n",
    "  activ = 1 / (1 + np.exp(-inp))\n",
    "  \n",
    "  return activ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDKs21WMnsB2"
   },
   "outputs": [],
   "source": [
    "def Sigmoid_backward(inp, gradient):\n",
    "  # Params, inp: Input to ReLU                                                 : (N,M)\n",
    "  #         gradient: Backpropogating gradient from the next layer             : (N,M)\n",
    "  # Output: gradient: gradient * Derivative of Sigmoid (sigmoid * (1-sigmoid)) : (N,M)\n",
    " \n",
    "  s = Sigmoid(inp)  \n",
    "  Dsigmoid = s*(1-s)\n",
    "  \n",
    "  out = gradient * Dsigmoid\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) TanH Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tL9tpCgpN9B"
   },
   "outputs": [],
   "source": [
    "def TanH(inp):\n",
    "  # Params, inp                                         : (N,M)\n",
    "  # Output, activ = (exp(2*inp) - 1) / (exp(2*inp) + 1) : (N,M)\n",
    "  \n",
    "  exp2a = np.exp(2*inp)\n",
    "  activ = (exp2a - 1) / (exp2a + 1)\n",
    "  \n",
    "  return activ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmWtJENpqb5U"
   },
   "outputs": [],
   "source": [
    "def TanH_backward(inp, gradient):\n",
    "  # Params, inp: Input to TanH                                                 : (N,M)\n",
    "  #         gradient: Backpropogating gradient from the next layer             : (N,M)\n",
    "  # Output: gradient: gradient * Derivative of TanH (1-TanH^2))                : (N,M)\n",
    "\n",
    "  tanh = TanH(inp)\n",
    "  Dtanh = 1 - (tanh**2)\n",
    "  \n",
    "  out = gradient * Dtanh\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) SoftMax Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlZAXV3SlMoS"
   },
   "outputs": [],
   "source": [
    "def softmax(inp):\n",
    "  # Params, inp                                        : (N,C)\n",
    "  # Output, out: probs = exp(inp_i) / Sum_j exp(inp_j) : (N,C)\n",
    "    \n",
    "  exp_inp = np.exp(inp)\n",
    "  probs = exp_inp / np.sum(exp_inp, axis=1, keepdims=True)\n",
    "  \n",
    "  return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yBV9l8ifh0Ja"
   },
   "outputs": [],
   "source": [
    "def softmax_backward(probs, GT):\n",
    "  # Params, probs: Network output probabilities                                                : (N,C)\n",
    "  #         GT:  each value belongs to one of C classes                                        : (N,)\n",
    "  # Output, Gradient of Pre-SoftMax activation with respect to output (dout = -(e(y) - probs)) : (N,C)          \n",
    "\n",
    "  dout = probs\n",
    "  dout[range(GT.shape[0]),GT] -= 1\n",
    "  dout /= GT.shape[0]\n",
    "\n",
    "  return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9I66WVX3g8jc"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(probs, GT):\n",
    "  # Params, probs: (N,C) where sum_c = 1\n",
    "  #         GT   : (N,) \n",
    "  # Output, loss : scalar\n",
    "  \n",
    "  # compute log probability for true value of GT for each example\n",
    "  logprobs = -np.log(probs[range(GT.shape[0]),GT])\n",
    "\n",
    "  loss = np.sum(logprobs)\n",
    "  loss /= GT.shape[0]\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Function to shuffle training data after epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fAwhNjj-apAv"
   },
   "outputs": [],
   "source": [
    "def shuffle_data(X,Y):\n",
    "   \n",
    "  s = np.arange(X.shape[0])\n",
    "  np.random.shuffle(s)\n",
    "\n",
    "  return X[s], Y[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Saving and Loading python objects as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S8WlMQ7lUufq"
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, path, name):\n",
    "    with open(path + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3NGOlFkVJAL"
   },
   "outputs": [],
   "source": [
    "def load_obj(path, name):\n",
    "    with open(path + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Neural Network Class Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that NN class has a function named initialization which allows us to initialize NN with any initialization method (glorot, normal, or zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fs9lThS8rZVH"
   },
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "\n",
    "  \n",
    "  def __init__(self,\n",
    "               input_dims=784, \n",
    "               output_dims=10, \n",
    "               hidden_dims=(1024,2048), \n",
    "               n_hidden=2, \n",
    "               regularization_coefficient=0,\n",
    "               init_method='zeros',           # 'zeros', 'normal', or 'glorot'\n",
    "               nonlinearity = 'relu',         # 'relu', 'tanh', or 'sigmoid'\n",
    "               LearningRate = 0.001):\n",
    "    \n",
    "    dims = (input_dims,) + hidden_dims + (output_dims,)    \n",
    "    self.n_hidden = n_hidden\n",
    "     \n",
    "    self.LR = LearningRate\n",
    "    self.reg_coeff = regularization_coefficient\n",
    "    \n",
    "    self.params = {}    # dictionary of parameters of NN\n",
    "    self.cache = {}     # dictionary of forward model cache\n",
    "    self.gradient = {}  # dictionary of backward model cache (gradient)\n",
    "\n",
    "    self.activ = nonlinearity\n",
    "    self.initialize_weights(n_hidden, dims, init_method)\n",
    "         \n",
    "        \n",
    "  def initialize_weights(self,n_hidden, dims, init_method):\n",
    "    \n",
    "    # W = weight parameters\n",
    "    # B = Biases\n",
    "    \n",
    "    for i in range(1,n_hidden+2):\n",
    "      \n",
    "      self.params['B'+str(i)] = np.zeros((1,dims[i]))\n",
    "      \n",
    "      if init_method == 'zeros':\n",
    "        self.params['W'+str(i)] = np.zeros((dims[i-1],dims[i]))\n",
    "      elif init_method == 'normal':\n",
    "        self.params['W'+str(i)] = np.random.normal(0.0, 1.0, (dims[i-1],dims[i]))        \n",
    "      elif init_method == 'glorot':\n",
    "        dl = np.sqrt(6/(dims[i-1]+dims[i]))\n",
    "        self.params['W'+str(i)] = np.random.uniform(-dl, dl, (dims[i-1],dims[i]))\n",
    "      else:\n",
    "        raise Exception('Weight Intialization Method should be one of the following: zeros, glorot, or normal') \n",
    "    \n",
    "    # DA = gradient of W\n",
    "    # DB = gradient pf B\n",
    "    for i in range(1,n_hidden+2):\n",
    "\n",
    "      self.gradient['DB'+str(i)] = np.zeros((1,dims[i]))     \n",
    "      self.gradient['DW'+str(i)] = np.zeros((dims[i-1],dims[i]))\n",
    "\n",
    "        \n",
    "  def activation(self,input):\n",
    "    \n",
    "    if self.activ == 'relu':\n",
    "      out = ReLU(input)\n",
    "    elif self.activ == 'tanh':\n",
    "      out = TanH(input)\n",
    "    elif self.activ == 'sigmoid':\n",
    "      out = Sigmoid(input)\n",
    "    else:\n",
    "      raise Exception('NonLinearity should be one of the following: relu, tanh, or sigmoid')\n",
    "        \n",
    "    return out\n",
    "        \n",
    "\n",
    "  def activation_backward(self,input,gradient):\n",
    "    \n",
    "    if self.activ == 'relu':\n",
    "      out = ReLU_backward(input, gradient)\n",
    "    elif self.activ == 'tanh':\n",
    "      out = TanH_backward(input, gradient)\n",
    "    elif self.activ == 'sigmoid':\n",
    "      out = Sigmoid_backward(input, gradient)\n",
    "    else:\n",
    "      raise Exception('NonLinearity should be one of the following: relu, tanh, or sigmoid')\n",
    "        \n",
    "    return out\n",
    "\n",
    "  \n",
    "  def forward(self,input):    \n",
    "    \n",
    "    # W = weight parameters\n",
    "    # B = Biases\n",
    "    # A = pre-activation (affine transformed input)\n",
    "    # H = post-activation (Hidden layer output)\n",
    "\n",
    "    self.cache['input'] = input\n",
    "    self.cache['A1'] = Affine_forward(self.cache['input'], self.params['W1'], self.params['B1']) # A1 = X*W1 + B1\n",
    "\n",
    "    for i in range(1,self.n_hidden+1):\n",
    "      self.cache['H'+str(i)] = self.activation(self.cache['A'+str(i)]) # Hi = active(Ai), ex: H1=active(A1)\n",
    "      self.cache['A'+str(i+1)] = Affine_forward(self.cache['H'+str(i)], self.params['W'+str(i+1)], self.params['B'+str(i+1)]) # A_(i+1) = H_(i)*W_(i+1) + B_(i+1), ex: A2 = H1*W2 + B2\n",
    "\n",
    "    self.cache['out'] = softmax(self.cache['A'+str(self.n_hidden+1)]) # out = softmax(A3)\n",
    "      \n",
    "    return self.cache['out']\n",
    "\n",
    "  \n",
    "  def loss(self,prediction,labels):\n",
    "\n",
    "    data_loss = cross_entropy_loss(prediction, labels)\n",
    "    \n",
    "    reg_loss = 0\n",
    "    \n",
    "    # if regularization parameter if greater than 0 than calculate L2 value of all weight layer to calculate reg_loss\n",
    "    if self.reg_coeff>0:\n",
    "      reg_loss = sum(np.sum(self.params['W'+str(i)]**2) for i in range(1,self.n_hidden+2))\n",
    "\n",
    "    loss = data_loss + (self.reg_coeff * reg_loss)\n",
    "    \n",
    "    return loss\n",
    "  \n",
    "  \n",
    "  def backward(self, labels):\n",
    "    \n",
    "    # DA is a dummy variable to store gradient of pre-activation\n",
    "    # DH is a dummy variable to store gradient of post-pactivation\n",
    "\n",
    "    DA = softmax_backward(self.cache['out'], labels) \n",
    "    \n",
    "    for i in range(self.n_hidden+1, 1, -1):\n",
    "      DH, self.gradient['DW'+str(i)], self.gradient['DB'+str(i)] =  Affine_backward(self.cache['H'+str(i-1)], self.params['W'+str(i)], self.params['B'+str(i)], DA) # DH2, DW3, DB3 = backward(H2, W3, B3)\n",
    "      DA = self.activation_backward(self.cache['A'+str(i-1)], DH) #DA2 = backward(A2, DH2)\n",
    " \n",
    "    _, self.gradient['DW1'], self.gradient['DB1'] = Affine_backward(self.cache['input'], self.params['W1'], self.params['B1'], DA) # Dinp, DW1, DB1 = backward(input, W1, B1, DA1)\n",
    "\n",
    "     \n",
    "  def update(self):\n",
    "    \n",
    "    # Update NN parameters\n",
    "    \n",
    "    for i in range(1,self.n_hidden+2):\n",
    "\n",
    "      self.params['B'+str(i)] -= self.LR*self.gradient['DB'+str(i)] \n",
    "      \n",
    "      if self.reg_coeff>0:\n",
    "        self.gradient['DW'+str(i)] += self.reg_coeff * self.params['W'+str(i)]\n",
    "\n",
    "      self.params['W'+str(i)] -= self.LR*self.gradient['DW'+str(i)]\n",
    "  \n",
    "  \n",
    "  def zero_gradient(self):\n",
    "    \n",
    "    # make gradients zero\n",
    "    # useful to do it at the starting of epocj\n",
    "\n",
    "    for i in range(1,n_hidden+2):\n",
    "\n",
    "      self.gradient['DB'+str(i)] = np.zeros(self.gradient['DB'+str(i)].shape)     \n",
    "      self.gradient['DW'+str(i)] = np.zeros(self.gradient['DW'+str(i)].shape)\n",
    "      \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function to train model for one epoch (pass through all training images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcCJ5vNQbQse"
   },
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, batch_size):\n",
    "    \n",
    "  samples = x_train.shape[0]\n",
    "\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for i in range(0, samples, batch_size):\n",
    "      \n",
    "    data, label = x_train[i:i+batch_size], y_train[i:i+batch_size] # get batch of data and label\n",
    "      \n",
    "    pred_probs = model.forward(data) # forward pass of the data\n",
    "      \n",
    "    batch_loss = model.loss(pred_probs, label) # calculate loss with respect to label\n",
    "      \n",
    "    epoch_loss += batch_loss # increment epoch loss\n",
    "      \n",
    "    model.backward(label) # backward pass with respect to label\n",
    "      \n",
    "    model.update() # update model parameters\n",
    "      \n",
    "    model.zero_gradient() # make gradient zero\n",
    "      \n",
    "  epoch_loss = (epoch_loss * batch_size) / (samples) # normalize epoch loss with respect to batch size and total samples\n",
    "    \n",
    "  return epoch_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function to get prediction for all test data and calculate loss and accuracy for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SeptjN7t9j6V"
   },
   "outputs": [],
   "source": [
    "def test(model, x, y):\n",
    "\n",
    "  model.zero_gradient() # make model gradient to zero\n",
    "  \n",
    "  samples = x.shape[0] # calculate total data samples\n",
    "  \n",
    "  epoch_loss = 0       # to store loss for the wholte dataset\n",
    "  true_prediction = 0  # to store total true prediction\n",
    "  \n",
    "  for i in range(0, samples):\n",
    "      \n",
    "    data, label = x[i:i+1], y[i:i+1] # get a sample of data\n",
    "      \n",
    "    pred_probs = model.forward(data) # forward pass through model\n",
    "    \n",
    "    pred_labels = np.argmax(pred_probs) # find the predicted label\n",
    "    \n",
    "    batch_loss = model.loss(pred_probs, label) # calculate loss\n",
    "\n",
    "    true_prediction += np.count_nonzero(pred_labels-label == 0) # check if prediction and true label are same or not\n",
    "    \n",
    "    epoch_loss += batch_loss # increment epoch loss\n",
    "            \n",
    "  epoch_loss = epoch_loss / samples # normalize loss with batch_size and samples\n",
    "  \n",
    "  accuracy = true_prediction / samples # calculate accuracy\n",
    "  \n",
    "  return epoch_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QG9ugMXk1p9y"
   },
   "outputs": [],
   "source": [
    "# experiment parameters\n",
    "total_epoch = 10\n",
    "\n",
    "data_path = 'mnist_dataset/mnist.npy'\n",
    "model_path = 'mnist_experiments/Exp_validate_gradient/'\n",
    "\n",
    "os.makedirs(model_path,exist_ok=True)\n",
    "\n",
    "input_dims=784 \n",
    "output_dims=10 \n",
    "hidden_dims=(512,1024) \n",
    "n_hidden=2 \n",
    "regularization_coefficient=0\n",
    "init_method ='glorot'          # 'zeros', 'normal', or 'glorot'\n",
    "nonlinearity = 'relu'         # 'relu', 'tanh', or 'sigmoid'\n",
    "LearningRate = 0.1\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read MNIST data and pre-process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rK2y5aiy4_ff"
   },
   "outputs": [],
   "source": [
    "# data-preparation\n",
    "(tr, va, te) = np.load(data_path)\n",
    "    \n",
    "x_train = tr[0]\n",
    "y_train = tr[1]\n",
    "x_valid = va[0]\n",
    "y_valid = va[1]\n",
    "x_test  = te[0]\n",
    "y_test  = te[1]\n",
    "    \n",
    "data_mean = x_train.mean()\n",
    "data_std  = x_train.std()\n",
    "    \n",
    "x_train = (x_train - data_mean) / data_std \n",
    "x_test  = (x_test  - data_mean) / data_std \n",
    "x_valid = (x_valid - data_mean) / data_std \n",
    "\n",
    "del tr, va, te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D8OJ0Ycs6tyd",
    "outputId": "f9f4c680-ed0e-4639-d0cb-9f2029c28e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Model Parameters:  937482\n"
     ]
    }
   ],
   "source": [
    "model = NN(input_dims, output_dims, hidden_dims, n_hidden, regularization_coefficient, init_method, nonlinearity, LearningRate)\n",
    "\n",
    "model_params = sum(value.size for _, value in model.params.items())\n",
    "\n",
    "print('Total Model Parameters: ', model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network for total_epochs and print and store training and validation loss and accuracy after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "3X4YHiBqBIOA",
    "outputId": "a6c1709d-a301-4bc3-c6c9-3d8ee316d2e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Training Done Epoch 1\n",
      "===> Training   Epoch 1: Loss - 0.0969, Accuracy - 0.9692\n",
      "===> Validation Epoch 1: Loss - 0.1185, Accuracy - 0.9651\n",
      "===> Model Saved Epoch 1\n",
      "===> Training Done Epoch 2\n",
      "===> Training   Epoch 2: Loss - 0.0593, Accuracy - 0.9799\n",
      "===> Validation Epoch 2: Loss - 0.1019, Accuracy - 0.9697\n",
      "===> Model Saved Epoch 2\n",
      "===> Training Done Epoch 3\n",
      "===> Training   Epoch 3: Loss - 0.0371, Accuracy - 0.9882\n",
      "===> Validation Epoch 3: Loss - 0.0862, Accuracy - 0.9747\n",
      "===> Model Saved Epoch 3\n",
      "===> Training Done Epoch 4\n",
      "===> Training   Epoch 4: Loss - 0.0217, Accuracy - 0.9923\n",
      "===> Validation Epoch 4: Loss - 0.0853, Accuracy - 0.9772\n",
      "===> Model Saved Epoch 4\n",
      "===> Training Done Epoch 5\n",
      "===> Training   Epoch 5: Loss - 0.0465, Accuracy - 0.9862\n",
      "===> Validation Epoch 5: Loss - 0.1171, Accuracy - 0.9733\n",
      "===> Model Saved Epoch 5\n",
      "===> Training Done Epoch 6\n",
      "===> Training   Epoch 6: Loss - 0.0295, Accuracy - 0.9916\n",
      "===> Validation Epoch 6: Loss - 0.1045, Accuracy - 0.9779\n",
      "===> Model Saved Epoch 6\n",
      "===> Training Done Epoch 7\n",
      "===> Training   Epoch 7: Loss - 0.0243, Accuracy - 0.9928\n",
      "===> Validation Epoch 7: Loss - 0.1052, Accuracy - 0.9777\n",
      "===> Model Saved Epoch 7\n",
      "===> Training Done Epoch 8\n",
      "===> Training   Epoch 8: Loss - 0.0100, Accuracy - 0.9964\n",
      "===> Validation Epoch 8: Loss - 0.0975, Accuracy - 0.9799\n",
      "===> Model Saved Epoch 8\n",
      "===> Training Done Epoch 9\n",
      "===> Training   Epoch 9: Loss - 0.0114, Accuracy - 0.9967\n",
      "===> Validation Epoch 9: Loss - 0.1018, Accuracy - 0.9780\n",
      "===> Model Saved Epoch 9\n",
      "===> Training Done Epoch 10\n",
      "===> Training   Epoch 10: Loss - 0.0038, Accuracy - 0.9989\n",
      "===> Validation Epoch 10: Loss - 0.0961, Accuracy - 0.9823\n",
      "===> Model Saved Epoch 10\n",
      "===> Testing Accuracy at the end of training: 0.9826\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "  \n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for i in range(1,total_epoch+1):\n",
    "  \n",
    "  loss = train(model, x_train, y_train, batch_size)\n",
    "  \n",
    "  print(\"===> Training Done Epoch {}\".format(i))\n",
    "  \n",
    "  loss, accuracy = test(model, x_train, y_train)\n",
    "  \n",
    "  train_loss = np.concatenate((train_loss, [loss]))\n",
    "  train_accuracy = np.concatenate((train_accuracy, [accuracy]))\n",
    "\n",
    "  print(\"===> Training   Epoch {}: Loss - {:.4f}, Accuracy - {:.4f}\".format(i, train_loss[-1], train_accuracy[-1]))\n",
    "\n",
    "  loss, accuracy = test(model, x_valid, y_valid)\n",
    "  \n",
    "  valid_loss = np.concatenate((valid_loss, [loss]))\n",
    "  valid_accuracy = np.concatenate((valid_accuracy, [accuracy]))\n",
    "\n",
    "  print(\"===> Validation Epoch {}: Loss - {:.4f}, Accuracy - {:.4f}\".format(i, valid_loss[-1], valid_accuracy[-1]))\n",
    "  \n",
    "  save_obj(model.params, model_path, 'model-{:02d}'.format(i))\n",
    "  \n",
    "  print(\"===> Model Saved Epoch {}\".format(i))\n",
    "\n",
    "  shuffle_data(x_train, y_train)\n",
    "\n",
    "  \n",
    "save_obj(train_loss, model_path, 'train-loss')\n",
    "save_obj(train_accuracy, model_path, 'train-accuracy')\n",
    "save_obj(valid_loss, model_path, 'valid-loss')\n",
    "save_obj(valid_accuracy, model_path, 'valid-accuracy')\n",
    "\n",
    "\n",
    "_, test_accuracy = test(model, x_test, y_test)\n",
    "\n",
    "print(\"===> Testing Accuracy at the end of training: {:.4f}\".format(test_accuracy) )\n",
    "\n",
    "save_obj([test_accuracy], model_path, 'test-accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4) Validate Gradient using finite difference at the end of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a single training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = x_train[1:2], y_train[1:2]\n",
    "      \n",
    "pred_probs = model.forward(data)\n",
    "                  \n",
    "model.backward(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store original params W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_W1 = model.params['W1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Maximum valur of i, j, and K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_i = 10\n",
    "max_j = 6\n",
    "max_k = 5\n",
    "\n",
    "Approx_delta = np.zeros((max_i, max_j, max_k)) #(10,6,5)\n",
    "\n",
    "True_delta = model.gradient['DW1'][0][0:max_i]\n",
    "\n",
    "True_delta = np.repeat(True_delta[:,np.newaxis],max_j,axis=-1)\n",
    "True_delta = np.repeat(True_delta[:,:,np.newaxis],max_k,axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### approximate gradient with finite difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,max_i):\n",
    "    for j in range(0,max_j):\n",
    "        for k in range(0,max_k):\n",
    "            N = (k+1)*(10**j)\n",
    "            epsi = 1 / N\n",
    "            ## positive\n",
    "            model.params['W1'][0][i] += epsi\n",
    "            pred_probs = model.forward(data)\n",
    "            loss = model.loss(pred_probs, label)\n",
    "            pos = loss\n",
    "            model.params['W1'] = orig_W1\n",
    "            ## negative\n",
    "            model.params['W1'][0][i] -= epsi\n",
    "            pred_probs = model.forward(data)\n",
    "            loss = model.loss(pred_probs, label)\n",
    "            neg = loss\n",
    "            model.params['W1'] = orig_W1\n",
    "            ## Approximate Delta\n",
    "            Approx_delta[i][j][k] = (pos-neg) / (2*epsi)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate maximum difference between finite difference and true difference for difference values of N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.zeros((max_j*max_k,))\n",
    "max_diff = np.zeros((max_j*max_k,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.e+00 2.e+00 3.e+00 4.e+00 5.e+00 1.e+01 2.e+01 3.e+01 4.e+01 5.e+01\n",
      " 1.e+02 2.e+02 3.e+02 4.e+02 5.e+02 1.e+03 2.e+03 3.e+03 4.e+03 5.e+03\n",
      " 1.e+04 2.e+04 3.e+04 4.e+04 5.e+04 1.e+05 2.e+05 3.e+05 4.e+05 5.e+05]\n",
      "[4.12914148e-15 3.96260802e-15 3.79607457e-15 3.85158572e-15\n",
      " 4.01811917e-15 3.18545191e-15 3.18545191e-15 2.63034039e-15\n",
      " 3.18545191e-15 2.07522888e-15 2.07522888e-15 0.00000000e+00\n",
      " 7.62634400e-15 7.62634400e-15 7.62634400e-15 7.62634400e-15\n",
      " 7.62634400e-15 7.62634400e-15 7.62634400e-15 7.62634400e-15\n",
      " 7.62634400e-15 7.62634400e-15 7.62634400e-15 7.62634400e-15\n",
      " 7.62634400e-15 7.62634400e-15 7.62634400e-15 7.62634400e-15\n",
      " 7.62634400e-15 7.62634400e-15]\n"
     ]
    }
   ],
   "source": [
    "#print(Approx_delta[9][0][0])\n",
    "#print(True_delta[9][0][0])\n",
    "i = 0\n",
    "for j in range(max_j):\n",
    "    for k in range(max_k):\n",
    "        N[i] = (k+1)*(10**j)\n",
    "        max_diff[i] = np.max(True_delta[:,j,k] - Approx_delta[:,j,k])\n",
    "        i = i+1\n",
    "    \n",
    "\n",
    "print(N)\n",
    "print(max_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot this value as a function of N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(0)\n",
    "plt.semilogx(N, max_diff)\n",
    "#plt.title('Maximum Difference between the true gradient and the finite gradient as a function of N')\n",
    "plt.grid(True)\n",
    "plt.xlabel('N=(k)*(10^j) where k={1,...,5}, j={0,...,5}')\n",
    "plt.ylabel('max_{1<=i<=10} |Delta_i^N - DL/DT_i|')\n",
    "#plt.show()\n",
    "plt.savefig('IFT6135H19_A1_Pc.png', dpi=500)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IFT6135H19_Assignment_1_P1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
