{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please run MNIST_download.ipynb to download, pre-process, and store all the necessary data in mnist_dataset folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W7UoFBoCFPKA",
    "outputId": "6af02037-73fe-4983-dd01-e57581659594"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IFT6135H19_A1_Pc.png\t\t  mnist_dataset\t\tmnist_experiments\r\n",
      "IFT6135H19_Assignment_1_P1.ipynb  MNIST_download.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFgiqz8yYmng"
   },
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/agoose77/numpy-html.git#egg=numpy-html\n",
    "#import numpy_html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "#np.set_printoptions(threshold=5, edgeitems=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1,2,3) Multi-Layer Perceptron in Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A) Helper Function for Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Affine Layer Forward and Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aYky0piO1K9T"
   },
   "outputs": [],
   "source": [
    "def Affine_forward(inp, W, B):\n",
    "  # Params, inp: Input to Layer : (NxD)\n",
    "  #         W: weight of Layer  : (DxM)\n",
    "  #         B  Bias of Layer    : (1xM)\n",
    "  # Output, out = inp*W + B     : (NxM) \n",
    "\n",
    "  out = np.dot(inp,W) + B  # out: (NxD)x(D,M) + (1,M) = (N,M)\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zk1CumTAx7mo"
   },
   "outputs": [],
   "source": [
    "def Affine_backward(inp, W, B, gradient):\n",
    "  # Assume, inp                         : (NxD)\n",
    "  #         W                           : (DxM)\n",
    "  #         B                           : (1XM)\n",
    "  #         gradient                    : (NxM)\n",
    "  #         reg regulazier scaler       : (1x1)  \n",
    "  # Output, Dinp = gradient*Traspose(T) : (NxD)\n",
    "  #         DW = Traspose(inp)*gradient : (NxD)\n",
    "  #         DB = Sum_N gradient         : (1xM) \n",
    "  \n",
    "  Dinp = np.dot(gradient, W.T)                  # DH: (NxM) * (MxD) = (NxD)\n",
    "  DW = np.dot(inp.T, gradient)                # DW  : (DxN) * (NxM) = (DxM)\n",
    "  DB = np.sum(gradient, 0, keepdims=True)     # DB  : Sum_N (N,M)   = (1xM)\n",
    "    \n",
    "  return Dinp, DW, DB\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) ReLU forward and backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "57WN3A_9l6z5"
   },
   "outputs": [],
   "source": [
    "def ReLU(inp):\n",
    "  # Params, inp               : (N,M)\n",
    "  # Output, activ = max(0,inp): (N,M)\n",
    "\n",
    "  activ = np.maximum(0,inp)\n",
    "  \n",
    "  return np.maximum(0,inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x78eTYjMuOCt"
   },
   "outputs": [],
   "source": [
    "def ReLU_backward(inp, gradient):\n",
    "  # Params, inp: Input to ReLU                                             : (N,M)\n",
    "  #         gradient: Backpropogating gradient from the next layer         : (N,M)\n",
    "  # Output: gradient: gradient * Derivative of ReLU (1 if X>0 otherwise 0) : (N,M)\n",
    "  \n",
    "  gradient[inp<=0] = 0\n",
    "  \n",
    "  return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Sigmoid Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DE7no9RdnHdI"
   },
   "outputs": [],
   "source": [
    "def Sigmoid(inp):\n",
    "  # Params, inp                       : (N,M)\n",
    "  # Output, activ = 1 / 1 + exp(-inp) : (N,M)\n",
    "  \n",
    "  activ = 1 / (1 + np.exp(-inp))\n",
    "  \n",
    "  return activ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDKs21WMnsB2"
   },
   "outputs": [],
   "source": [
    "def Sigmoid_backward(inp, gradient):\n",
    "  # Params, inp: Input to ReLU                                                 : (N,M)\n",
    "  #         gradient: Backpropogating gradient from the next layer             : (N,M)\n",
    "  # Output: gradient: gradient * Derivative of Sigmoid (sigmoid * (1-sigmoid)) : (N,M)\n",
    " \n",
    "  s = Sigmoid(inp)  \n",
    "  Dsigmoid = s*(1-s)\n",
    "  \n",
    "  out = gradient * Dsigmoid\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) TanH Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1tL9tpCgpN9B"
   },
   "outputs": [],
   "source": [
    "def TanH(inp):\n",
    "  # Params, inp                                         : (N,M)\n",
    "  # Output, activ = (exp(2*inp) - 1) / (exp(2*inp) + 1) : (N,M)\n",
    "  \n",
    "  exp2a = np.exp(2*inp)\n",
    "  activ = (exp2a - 1) / (exp2a + 1)\n",
    "  \n",
    "  return activ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OmWtJENpqb5U"
   },
   "outputs": [],
   "source": [
    "def TanH_backward(inp, gradient):\n",
    "  # Params, inp: Input to TanH                                                 : (N,M)\n",
    "  #         gradient: Backpropogating gradient from the next layer             : (N,M)\n",
    "  # Output: gradient: gradient * Derivative of TanH (1-TanH^2))                : (N,M)\n",
    "\n",
    "  tanh = TanH(inp)\n",
    "  Dtanh = 1 - (tanh**2)\n",
    "  \n",
    "  out = gradient * Dtanh\n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) SoftMax Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlZAXV3SlMoS"
   },
   "outputs": [],
   "source": [
    "def softmax(inp):\n",
    "  # Params, inp                                        : (N,C)\n",
    "  # Output, out: probs = exp(inp_i) / Sum_j exp(inp_j) : (N,C)\n",
    "    \n",
    "  exp_inp = np.exp(inp)\n",
    "  probs = exp_inp / np.sum(exp_inp, axis=1, keepdims=True)\n",
    "  \n",
    "  return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yBV9l8ifh0Ja"
   },
   "outputs": [],
   "source": [
    "def softmax_backward(probs, GT):\n",
    "  # Params, probs: Network output probabilities                                                : (N,C)\n",
    "  #         GT:  each value belongs to one of C classes                                        : (N,)\n",
    "  # Output, Gradient of Pre-SoftMax activation with respect to output (dout = -(e(y) - probs)) : (N,C)          \n",
    "\n",
    "  dout = probs\n",
    "  dout[range(GT.shape[0]),GT] -= 1\n",
    "  dout /= GT.shape[0]\n",
    "\n",
    "  return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9I66WVX3g8jc"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(probs, GT):\n",
    "  # Params, probs: (N,C) where sum_c = 1\n",
    "  #         GT   : (N,) \n",
    "  # Output, loss : scalar\n",
    "  \n",
    "  # compute log probability for true value of GT for each example\n",
    "  logprobs = -np.log(probs[range(GT.shape[0]),GT])\n",
    "\n",
    "  loss = np.sum(logprobs)\n",
    "  loss /= GT.shape[0]\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Function to shuffle training data after epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fAwhNjj-apAv"
   },
   "outputs": [],
   "source": [
    "def shuffle_data(X,Y):\n",
    "   \n",
    "  s = np.arange(X.shape[0])\n",
    "  np.random.shuffle(s)\n",
    "\n",
    "  return X[s], Y[s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Saving and Loading python objects as pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S8WlMQ7lUufq"
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, path, name):\n",
    "    with open(path + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3NGOlFkVJAL"
   },
   "outputs": [],
   "source": [
    "def load_obj(path, name):\n",
    "    with open(path + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B) Neural Network Class Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that NN class has a function named initialization which allows us to initialize NN with any initialization method (glorot, normal, or zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fs9lThS8rZVH"
   },
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "\n",
    "  \n",
    "  def __init__(self,\n",
    "               input_dims=784, \n",
    "               output_dims=10, \n",
    "               hidden_dims=(1024,2048), \n",
    "               n_hidden=2, \n",
    "               regularization_coefficient=0,\n",
    "               init_method='zeros',           # 'zeros', 'normal', or 'glorot'\n",
    "               nonlinearity = 'relu',         # 'relu', 'tanh', or 'sigmoid'\n",
    "               LearningRate = 0.001):\n",
    "    \n",
    "    dims = (input_dims,) + hidden_dims + (output_dims,)    \n",
    "    self.n_hidden = n_hidden\n",
    "     \n",
    "    self.LR = LearningRate\n",
    "    self.reg_coeff = regularization_coefficient\n",
    "    \n",
    "    self.params = {}    # dictionary of parameters of NN\n",
    "    self.cache = {}     # dictionary of forward model cache\n",
    "    self.gradient = {}  # dictionary of backward model cache (gradient)\n",
    "\n",
    "    self.activ = nonlinearity\n",
    "    self.initialize_weights(n_hidden, dims, init_method)\n",
    "         \n",
    "        \n",
    "  def initialize_weights(self,n_hidden, dims, init_method):\n",
    "    \n",
    "    # W = weight parameters\n",
    "    # B = Biases\n",
    "    \n",
    "    for i in range(1,n_hidden+2):\n",
    "      \n",
    "      self.params['B'+str(i)] = np.zeros((1,dims[i]))\n",
    "      \n",
    "      if init_method == 'zeros':\n",
    "        self.params['W'+str(i)] = np.zeros((dims[i-1],dims[i]))\n",
    "      elif init_method == 'normal':\n",
    "        self.params['W'+str(i)] = np.random.normal(0.0, 1.0, (dims[i-1],dims[i]))        \n",
    "      elif init_method == 'glorot':\n",
    "        dl = np.sqrt(6/(dims[i-1]+dims[i]))\n",
    "        self.params['W'+str(i)] = np.random.uniform(-dl, dl, (dims[i-1],dims[i]))\n",
    "      else:\n",
    "        raise Exception('Weight Intialization Method should be one of the following: zeros, glorot, or normal') \n",
    "    \n",
    "    # DA = gradient of W\n",
    "    # DB = gradient pf B\n",
    "    for i in range(1,n_hidden+2):\n",
    "\n",
    "      self.gradient['DB'+str(i)] = np.zeros((1,dims[i]))     \n",
    "      self.gradient['DW'+str(i)] = np.zeros((dims[i-1],dims[i]))\n",
    "\n",
    "        \n",
    "  def activation(self,input):\n",
    "    \n",
    "    if self.activ == 'relu':\n",
    "      out = ReLU(input)\n",
    "    elif self.activ == 'tanh':\n",
    "      out = TanH(input)\n",
    "    elif self.activ == 'sigmoid':\n",
    "      out = Sigmoid(input)\n",
    "    else:\n",
    "      raise Exception('NonLinearity should be one of the following: relu, tanh, or sigmoid')\n",
    "        \n",
    "    return out\n",
    "        \n",
    "\n",
    "  def activation_backward(self,input,gradient):\n",
    "    \n",
    "    if self.activ == 'relu':\n",
    "      out = ReLU_backward(input, gradient)\n",
    "    elif self.activ == 'tanh':\n",
    "      out = TanH_backward(input, gradient)\n",
    "    elif self.activ == 'sigmoid':\n",
    "      out = Sigmoid_backward(input, gradient)\n",
    "    else:\n",
    "      raise Exception('NonLinearity should be one of the following: relu, tanh, or sigmoid')\n",
    "        \n",
    "    return out\n",
    "\n",
    "  \n",
    "  def forward(self,input):    \n",
    "    \n",
    "    # W = weight parameters\n",
    "    # B = Biases\n",
    "    # A = pre-activation (affine transformed input)\n",
    "    # H = post-activation (Hidden layer output)\n",
    "\n",
    "    self.cache['input'] = input\n",
    "    self.cache['A1'] = Affine_forward(self.cache['input'], self.params['W1'], self.params['B1']) # A1 = X*W1 + B1\n",
    "\n",
    "    for i in range(1,self.n_hidden+1):\n",
    "      self.cache['H'+str(i)] = self.activation(self.cache['A'+str(i)]) # Hi = active(Ai), ex: H1=active(A1)\n",
    "      self.cache['A'+str(i+1)] = Affine_forward(self.cache['H'+str(i)], self.params['W'+str(i+1)], self.params['B'+str(i+1)]) # A_(i+1) = H_(i)*W_(i+1) + B_(i+1), ex: A2 = H1*W2 + B2\n",
    "\n",
    "    self.cache['out'] = softmax(self.cache['A'+str(self.n_hidden+1)]) # out = softmax(A3)\n",
    "      \n",
    "    return self.cache['out']\n",
    "\n",
    "  \n",
    "  def loss(self,prediction,labels):\n",
    "\n",
    "    data_loss = cross_entropy_loss(prediction, labels)\n",
    "    \n",
    "    reg_loss = 0\n",
    "    \n",
    "    # if regularization parameter if greater than 0 than calculate L2 value of all weight layer to calculate reg_loss\n",
    "    if self.reg_coeff>0:\n",
    "      reg_loss = sum(np.sum(self.params['W'+str(i)]**2) for i in range(1,self.n_hidden+2))\n",
    "\n",
    "    loss = data_loss + (self.reg_coeff * reg_loss)\n",
    "    \n",
    "    return loss\n",
    "  \n",
    "  \n",
    "  def backward(self, labels):\n",
    "    \n",
    "    # DA is a dummy variable to store gradient of pre-activation\n",
    "    # DH is a dummy variable to store gradient of post-pactivation\n",
    "\n",
    "    DA = softmax_backward(self.cache['out'], labels) \n",
    "    \n",
    "    for i in range(self.n_hidden+1, 1, -1):\n",
    "      DH, self.gradient['DW'+str(i)], self.gradient['DB'+str(i)] =  Affine_backward(self.cache['H'+str(i-1)], self.params['W'+str(i)], self.params['B'+str(i)], DA) # DH2, DW3, DB3 = backward(H2, W3, B3)\n",
    "      DA = self.activation_backward(self.cache['A'+str(i-1)], DH) #DA2 = backward(A2, DH2)\n",
    " \n",
    "    _, self.gradient['DW1'], self.gradient['DB1'] = Affine_backward(self.cache['input'], self.params['W1'], self.params['B1'], DA) # Dinp, DW1, DB1 = backward(input, W1, B1, DA1)\n",
    "\n",
    "     \n",
    "  def update(self):\n",
    "    \n",
    "    # Update NN parameters\n",
    "    \n",
    "    for i in range(1,self.n_hidden+2):\n",
    "\n",
    "      self.params['B'+str(i)] -= self.LR*self.gradient['DB'+str(i)] \n",
    "      \n",
    "      if self.reg_coeff>0:\n",
    "        self.gradient['DW'+str(i)] += self.reg_coeff * self.params['W'+str(i)]\n",
    "\n",
    "      self.params['W'+str(i)] -= self.LR*self.gradient['DW'+str(i)]\n",
    "  \n",
    "  \n",
    "  def zero_gradient(self):\n",
    "    \n",
    "    # make gradients zero\n",
    "    # useful to do it at the starting of epocj\n",
    "\n",
    "    for i in range(1,n_hidden+2):\n",
    "\n",
    "      self.gradient['DB'+str(i)] = np.zeros(self.gradient['DB'+str(i)].shape)     \n",
    "      self.gradient['DW'+str(i)] = np.zeros(self.gradient['DW'+str(i)].shape)\n",
    "      \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function to train model for one epoch (pass through all training images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcCJ5vNQbQse"
   },
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, batch_size):\n",
    "    \n",
    "  samples = x_train.shape[0]\n",
    "\n",
    "  epoch_loss = 0\n",
    "\n",
    "  for i in range(0, samples, batch_size):\n",
    "      \n",
    "    data, label = x_train[i:i+batch_size], y_train[i:i+batch_size] # get batch of data and label\n",
    "      \n",
    "    pred_probs = model.forward(data) # forward pass of the data\n",
    "      \n",
    "    batch_loss = model.loss(pred_probs, label) # calculate loss with respect to label\n",
    "      \n",
    "    epoch_loss += batch_loss # increment epoch loss\n",
    "      \n",
    "    model.backward(label) # backward pass with respect to label\n",
    "      \n",
    "    model.update() # update model parameters\n",
    "      \n",
    "    model.zero_gradient() # make gradient zero\n",
    "      \n",
    "  epoch_loss = (epoch_loss * batch_size) / (samples) # normalize epoch loss with respect to batch size and total samples\n",
    "    \n",
    "  return epoch_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function to get prediction for all test data and calculate loss and accuracy for this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SeptjN7t9j6V"
   },
   "outputs": [],
   "source": [
    "def test(model, x, y):\n",
    "\n",
    "  model.zero_gradient() # make model gradient to zero\n",
    "  \n",
    "  samples = x.shape[0] # calculate total data samples\n",
    "  \n",
    "  epoch_loss = 0       # to store loss for the wholte dataset\n",
    "  true_prediction = 0  # to store total true prediction\n",
    "  \n",
    "  for i in range(0, samples):\n",
    "      \n",
    "    data, label = x[i:i+1], y[i:i+1] # get a sample of data\n",
    "      \n",
    "    pred_probs = model.forward(data) # forward pass through model\n",
    "    \n",
    "    pred_labels = np.argmax(pred_probs) # find the predicted label\n",
    "    \n",
    "    batch_loss = model.loss(pred_probs, label) # calculate loss\n",
    "\n",
    "    true_prediction += np.count_nonzero(pred_labels-label == 0) # check if prediction and true label are same or not\n",
    "    \n",
    "    epoch_loss += batch_loss # increment epoch loss\n",
    "            \n",
    "  epoch_loss = epoch_loss / samples # normalize loss with batch_size and samples\n",
    "  \n",
    "  accuracy = true_prediction / samples # calculate accuracy\n",
    "  \n",
    "  return epoch_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QG9ugMXk1p9y"
   },
   "outputs": [],
   "source": [
    "# experiment parameters\n",
    "total_epoch = 10\n",
    "\n",
    "data_path = 'mnist_dataset/mnist.npy'\n",
    "model_path = 'mnist_experiments/Exp_validate_gradient/'\n",
    "\n",
    "os.makedirs(model_path,exist_ok=True)\n",
    "\n",
    "input_dims=784 \n",
    "output_dims=10 \n",
    "hidden_dims=(128,64) \n",
    "n_hidden=2 \n",
    "regularization_coefficient=0\n",
    "init_method ='glorot'          # 'zeros', 'normal', or 'glorot'\n",
    "nonlinearity = 'relu'         # 'relu', 'tanh', or 'sigmoid'\n",
    "LearningRate = 0.1\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read MNIST data and pre-process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rK2y5aiy4_ff"
   },
   "outputs": [],
   "source": [
    "# data-preparation\n",
    "(tr, va, te) = np.load(data_path)\n",
    "    \n",
    "x_train = tr[0]\n",
    "y_train = tr[1]\n",
    "x_valid = va[0]\n",
    "y_valid = va[1]\n",
    "x_test  = te[0]\n",
    "y_test  = te[1]\n",
    "    \n",
    "data_mean = x_train.mean()\n",
    "data_std  = x_train.std()\n",
    "    \n",
    "x_train = (x_train - data_mean) / data_std \n",
    "x_test  = (x_test  - data_mean) / data_std \n",
    "x_valid = (x_valid - data_mean) / data_std \n",
    "\n",
    "del tr, va, te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "D8OJ0Ycs6tyd",
    "outputId": "f9f4c680-ed0e-4639-d0cb-9f2029c28e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Model Parameters:  109386\n"
     ]
    }
   ],
   "source": [
    "model = NN(input_dims, output_dims, hidden_dims, n_hidden, regularization_coefficient, init_method, nonlinearity, LearningRate)\n",
    "\n",
    "model_params = sum(value.size for _, value in model.params.items())\n",
    "\n",
    "print('Total Model Parameters: ', model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the network for total_epochs and print and store training and validation loss and accuracy after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "3X4YHiBqBIOA",
    "outputId": "a6c1709d-a301-4bc3-c6c9-3d8ee316d2e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Training Done Epoch 1\n",
      "===> Training   Epoch 1: Loss - 0.1311, Accuracy - 0.9596\n",
      "===> Validation Epoch 1: Loss - 0.1435, Accuracy - 0.9580\n",
      "===> Model Saved Epoch 1\n",
      "===> Training Done Epoch 2\n",
      "===> Training   Epoch 2: Loss - 0.1093, Accuracy - 0.9643\n",
      "===> Validation Epoch 2: Loss - 0.1519, Accuracy - 0.9578\n",
      "===> Model Saved Epoch 2\n",
      "===> Training Done Epoch 3\n",
      "===> Training   Epoch 3: Loss - 0.0799, Accuracy - 0.9742\n",
      "===> Validation Epoch 3: Loss - 0.1353, Accuracy - 0.9643\n",
      "===> Model Saved Epoch 3\n",
      "===> Training Done Epoch 4\n",
      "===> Training   Epoch 4: Loss - 0.0790, Accuracy - 0.9756\n",
      "===> Validation Epoch 4: Loss - 0.1513, Accuracy - 0.9648\n",
      "===> Model Saved Epoch 4\n",
      "===> Training Done Epoch 5\n",
      "===> Training   Epoch 5: Loss - 0.0632, Accuracy - 0.9810\n",
      "===> Validation Epoch 5: Loss - 0.1383, Accuracy - 0.9683\n",
      "===> Model Saved Epoch 5\n",
      "===> Training Done Epoch 6\n",
      "===> Training   Epoch 6: Loss - 0.0384, Accuracy - 0.9880\n",
      "===> Validation Epoch 6: Loss - 0.1337, Accuracy - 0.9694\n",
      "===> Model Saved Epoch 6\n",
      "===> Training Done Epoch 7\n",
      "===> Training   Epoch 7: Loss - 0.0649, Accuracy - 0.9805\n",
      "===> Validation Epoch 7: Loss - 0.1684, Accuracy - 0.9632\n",
      "===> Model Saved Epoch 7\n",
      "===> Training Done Epoch 8\n",
      "===> Training   Epoch 8: Loss - 0.0678, Accuracy - 0.9817\n",
      "===> Validation Epoch 8: Loss - 0.1780, Accuracy - 0.9654\n",
      "===> Model Saved Epoch 8\n",
      "===> Training Done Epoch 9\n",
      "===> Training   Epoch 9: Loss - 0.0433, Accuracy - 0.9864\n",
      "===> Validation Epoch 9: Loss - 0.1516, Accuracy - 0.9699\n",
      "===> Model Saved Epoch 9\n",
      "===> Training Done Epoch 10\n",
      "===> Training   Epoch 10: Loss - 0.0498, Accuracy - 0.9856\n",
      "===> Validation Epoch 10: Loss - 0.1680, Accuracy - 0.9692\n",
      "===> Model Saved Epoch 10\n",
      "===> Testing Accuracy at the end of training: 0.9669\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "  \n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "\n",
    "for i in range(1,total_epoch+1):\n",
    "  \n",
    "  loss = train(model, x_train, y_train, batch_size)\n",
    "  \n",
    "  print(\"===> Training Done Epoch {}\".format(i))\n",
    "  \n",
    "  loss, accuracy = test(model, x_train, y_train)\n",
    "  \n",
    "  train_loss = np.concatenate((train_loss, [loss]))\n",
    "  train_accuracy = np.concatenate((train_accuracy, [accuracy]))\n",
    "\n",
    "  print(\"===> Training   Epoch {}: Loss - {:.4f}, Accuracy - {:.4f}\".format(i, train_loss[-1], train_accuracy[-1]))\n",
    "\n",
    "  loss, accuracy = test(model, x_valid, y_valid)\n",
    "  \n",
    "  valid_loss = np.concatenate((valid_loss, [loss]))\n",
    "  valid_accuracy = np.concatenate((valid_accuracy, [accuracy]))\n",
    "\n",
    "  print(\"===> Validation Epoch {}: Loss - {:.4f}, Accuracy - {:.4f}\".format(i, valid_loss[-1], valid_accuracy[-1]))\n",
    "  \n",
    "  save_obj(model.params, model_path, 'model-{:02d}'.format(i))\n",
    "  \n",
    "  print(\"===> Model Saved Epoch {}\".format(i))\n",
    "\n",
    "  shuffle_data(x_train, y_train)\n",
    "\n",
    "  \n",
    "save_obj(train_loss, model_path, 'train-loss')\n",
    "save_obj(train_accuracy, model_path, 'train-accuracy')\n",
    "save_obj(valid_loss, model_path, 'valid-loss')\n",
    "save_obj(valid_accuracy, model_path, 'valid-accuracy')\n",
    "\n",
    "\n",
    "_, test_accuracy = test(model, x_test, y_test)\n",
    "\n",
    "print(\"===> Testing Accuracy at the end of training: {:.4f}\".format(test_accuracy) )\n",
    "\n",
    "save_obj([test_accuracy], model_path, 'test-accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-4) Validate Gradient using finite difference at the end of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a single training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = x_train[1:2], y_train[1:2]\n",
    "      \n",
    "pred_probs = model.forward(data)\n",
    "                  \n",
    "model.backward(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store original params W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_W2 = model.params['W2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Maximum valur of i, j, and K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_i = 10 # first 10-elements of second layer weight\n",
    "max_j = 6 # j = {0,1,2,3,4,5}\n",
    "max_k = 5 # k = {1,2,3,4,5}\n",
    "\n",
    "Approx_delta = np.zeros((max_i, max_j, max_k)) #(10,6,5)\n",
    "\n",
    "True_delta = model.gradient['DW2'][50][0:max_i]\n",
    "True_delta = np.repeat(True_delta[:,np.newaxis],max_j,axis=-1)\n",
    "True_delta = np.repeat(True_delta[:,:,np.newaxis],max_k,axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.63137072e-10  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -5.89692210e-10\n",
      "  0.00000000e+00  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(model.gradient['DW2'][50][0:max_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### approximate gradient with finite difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,max_i):\n",
    "    for j in range(0,max_j):\n",
    "        for k in range(0,max_k):\n",
    "            N = (k+1)*(10**j)\n",
    "            epsi = 1 / N\n",
    "            ## positive\n",
    "            model.params['W2'][50][i] += epsi\n",
    "            pred_probs = model.forward(data)\n",
    "            loss = model.loss(pred_probs, label)\n",
    "            pos = loss\n",
    "            model.params['W2'] = orig_W2\n",
    "            ## negative\n",
    "            model.params['W2'][50][i] -= epsi\n",
    "            pred_probs = model.forward(data)\n",
    "            loss = model.loss(pred_probs, label)\n",
    "            neg = loss\n",
    "            model.params['W2'] = orig_W2\n",
    "            ## Approximate Delta\n",
    "            Approx_delta[i][j][k] = (pos-neg) / (2*epsi)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate maximum difference between finite difference and true difference for difference values of N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.zeros((max_j*max_k,))\n",
    "max_diff = np.zeros((max_j*max_k,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.e+00 2.e+00 3.e+00 4.e+00 5.e+00 1.e+01 2.e+01 3.e+01 4.e+01 5.e+01\n",
      " 1.e+02 2.e+02 3.e+02 4.e+02 5.e+02 1.e+03 2.e+03 3.e+03 4.e+03 5.e+03\n",
      " 1.e+04 2.e+04 3.e+04 4.e+04 5.e+04 1.e+05 2.e+05 3.e+05 4.e+05 5.e+05]\n",
      "[5.49653656e-11 3.80786514e-11 1.75304771e-11 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "#print(Approx_delta[9][0][0])\n",
    "#print(True_delta[9][0][0])\n",
    "i = 0\n",
    "for j in range(max_j):\n",
    "    for k in range(max_k):\n",
    "        N[i] = (k+1)*(10**j)\n",
    "        max_diff[i] = np.max(True_delta[:,j,k] - Approx_delta[:,j,k])\n",
    "        i = i+1\n",
    "    \n",
    "\n",
    "print(N)\n",
    "print(max_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot this value as a function of N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(0)\n",
    "plt.semilogx(N, max_diff)\n",
    "#plt.title('Maximum Difference between the true gradient and the finite gradient as a function of N')\n",
    "plt.grid(True)\n",
    "plt.xlabel('N=(k)*(10^j) where k={1,...,5}, j={0,...,5}')\n",
    "plt.ylabel('max_{1<=i<=10} |Delta_i^N - DL/DT_i|')\n",
    "#plt.show()\n",
    "plt.savefig('IFT6135H19_A1_Pc.png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-2.63137072e-10 -2.63137072e-10 -2.63137072e-10 -2.63137072e-10\n",
      "   -2.63137072e-10]\n",
      "  [-2.63137072e-10 -2.63137072e-10 -2.63137072e-10 -2.63137072e-10\n",
      "   -2.63137072e-10]\n",
      "  [-2.63137072e-10 -2.63137072e-10 -2.63137072e-10 -2.63137072e-10\n",
      "   -2.63137072e-10]\n",
      "  [-2.63137072e-10 -2.63137072e-10 -2.63137072e-10 -2.63137072e-10\n",
      "   -2.63137072e-10]\n",
      "  [-2.63137072e-10 -2.63137072e-10 -2.63137072e-10 -2.63137072e-10\n",
      "   -2.63137072e-10]\n",
      "  [-2.63137072e-10 -2.63137072e-10 -2.63137072e-10 -2.63137072e-10\n",
      "   -2.63137072e-10]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[-5.89692210e-10 -5.89692210e-10 -5.89692210e-10 -5.89692210e-10\n",
      "   -5.89692210e-10]\n",
      "  [-5.89692210e-10 -5.89692210e-10 -5.89692210e-10 -5.89692210e-10\n",
      "   -5.89692210e-10]\n",
      "  [-5.89692210e-10 -5.89692210e-10 -5.89692210e-10 -5.89692210e-10\n",
      "   -5.89692210e-10]\n",
      "  [-5.89692210e-10 -5.89692210e-10 -5.89692210e-10 -5.89692210e-10\n",
      "   -5.89692210e-10]\n",
      "  [-5.89692210e-10 -5.89692210e-10 -5.89692210e-10 -5.89692210e-10\n",
      "   -5.89692210e-10]\n",
      "  [-5.89692210e-10 -5.89692210e-10 -5.89692210e-10 -5.89692210e-10\n",
      "   -5.89692210e-10]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "print(True_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.06713527e-10 -1.18249077e-10 -1.22475530e-10 -1.24666499e-10\n",
      "   -1.26006983e-10]\n",
      "  [-1.28746458e-10 -1.30145894e-10 -1.30618849e-10 -1.30853106e-10\n",
      "   -1.30995215e-10]\n",
      "  [-1.31278322e-10 -1.31417100e-10 -1.31461508e-10 -1.31472611e-10\n",
      "   -1.31505917e-10]\n",
      "  [-1.31561428e-10 -1.31339384e-10 -1.31561428e-10 -1.31450406e-10\n",
      "   -1.31283873e-10]\n",
      "  [-1.30451205e-10 -1.29896094e-10 -1.31561428e-10 -1.31006317e-10\n",
      "   -1.30451205e-10]\n",
      "  [-1.22124533e-10 -1.22124533e-10 -1.33226763e-10 -1.33226763e-10\n",
      "   -1.11022303e-10]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[-7.76118059e-12  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[-5.49653656e-11 -3.80786514e-11 -1.75304771e-11  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[-1.88263183e-10 -2.33280173e-10 -2.51662025e-10 -2.61606292e-10\n",
      "   -2.67832145e-10]\n",
      "  [-2.80901413e-10 -2.87760926e-10 -2.90094615e-10 -2.91273672e-10\n",
      "   -2.91985880e-10]\n",
      "  [-2.93415292e-10 -2.94109182e-10 -2.94364533e-10 -2.94475555e-10\n",
      "   -2.94514413e-10]\n",
      "  [-2.94708702e-10 -2.94653191e-10 -2.94431146e-10 -2.94431146e-10\n",
      "   -2.94486657e-10]\n",
      "  [-2.94209102e-10 -2.94209102e-10 -2.94764213e-10 -2.93098879e-10\n",
      "   -2.91433544e-10]\n",
      "  [-2.88657987e-10 -2.77555756e-10 -2.66453526e-10 -2.66453526e-10\n",
      "   -2.49800181e-10]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "\n",
      " [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]\n",
      "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "    0.00000000e+00]]]\n"
     ]
    }
   ],
   "source": [
    "print(Approx_delta)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IFT6135H19_Assignment_1_P1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
