{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ltpuCeFFCTQA"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import utils\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from scipy import stats\n",
    "from torch.functional import F\n",
    "from torch.optim import Adam\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "D-JHH1YTnV3X",
    "outputId": "1a10f0be-16c0-4e5c-e2c6-ead9ed5af23b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "path = '/content/gdrive/My Drive/RepLearning/VAE/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uVL8uWrngdnh"
   },
   "source": [
    "### References: \n",
    "https://github.com/CW-Huang/IFT6135H19_assignment/blob/master/assignment3/mnist_loader.ipynb\n",
    "\n",
    "https://github.com/pytorch/examples/blob/master/vae/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDDfWgBCj3Hq"
   },
   "source": [
    "# Loading BinaryMNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFOePpNFCZig"
   },
   "outputs": [],
   "source": [
    "def get_data_loader(dataset_location, batch_size):\n",
    "    URL = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/\"\n",
    "    # start processing\n",
    "    def lines_to_np_array(lines):\n",
    "        return np.array([[int(i) for i in line.split()] for line in lines])\n",
    "    splitdata = []\n",
    "    for splitname in [\"train\", \"valid\", \"test\"]:\n",
    "        filename = \"binarized_mnist_%s.amat\" % splitname\n",
    "        filepath = os.path.join(dataset_location, filename)\n",
    "        utils.download_url(URL + filename, dataset_location)\n",
    "        with open(filepath) as f:\n",
    "            lines = f.readlines()\n",
    "        x = lines_to_np_array(lines).astype('float32')\n",
    "        x = x.reshape(x.shape[0], 1, 28, 28)\n",
    "        # pytorch data loader\n",
    "        dataset = data_utils.TensorDataset(torch.from_numpy(x))\n",
    "        dataset_loader = data_utils.DataLoader(x, batch_size=batch_size, shuffle=splitname == \"train\")\n",
    "        splitdata.append(dataset_loader)\n",
    "    return splitdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwBAB70GPFVb"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cpu\") # \"cuda\"\n",
    "n_epochs = 20\n",
    "log_interval = 100\n",
    "batch_size = 64\n",
    "latent_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "a8Mu4_itCbPS",
    "outputId": "c7c32fff-5cc9-4119-b77b-9e3fe326c7aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: binarized_mnist/binarized_mnist_train.amat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "15687680it [00:20, 4296714.20it/s]                              \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: binarized_mnist/binarized_mnist_valid.amat\n",
      "Using downloaded and verified file: binarized_mnist/binarized_mnist_test.amat\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader = get_data_loader(\"binarized_mnist\", batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "mY4uyTtmCdD8",
    "outputId": "eb65430d-242a-4cc4-e606-bf366be48359"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nv9SU8UtN/T/9RIEW8OOhLQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x in train_loader:\n",
    "    plt.imshow(x[0, 0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "94KCZtsbCkyK",
    "outputId": "09d145c3-c8eb-44e0-9b11-5423797b9ad7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  782 \n",
      "valid:  157 \n",
      "test:   157\n"
     ]
    }
   ],
   "source": [
    "print('train: ', len(train_loader), '\\nvalid: ', len(valid_loader), '\\ntest:  ', len(test_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2XAvxDzsi9KR"
   },
   "source": [
    "# VAE Model\n",
    "Uses the outlined architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZFYXDlnU88m"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # encoder layers\n",
    "        self.conv1 = nn.Conv2d(1  , 32 , (3, 3))\n",
    "        self.conv2 = nn.Conv2d(32 , 64 , (3, 3))\n",
    "        self.conv3 = nn.Conv2d(64 , 256, (5, 5))\n",
    "        self.fc1   = nn.Linear(256, 2*latent_size)\n",
    "        \n",
    "        # decoder layers\n",
    "        self.fc2   = nn.Linear(latent_size, 256)\n",
    "        self.conv4 = nn.Conv2d(256, 64, (5, 5), padding=(4,4))\n",
    "        self.conv5 = nn.Conv2d(64 , 32, (3, 3), padding=(2,2))\n",
    "        self.conv6 = nn.Conv2d(32 , 16, (3, 3), padding=(2,2))        \n",
    "        self.conv7 = nn.Conv2d(16 , 1 , (3, 3), padding=(2,2))\n",
    "        self.upsmple = nn.UpsamplingBilinear2d(scale_factor=2) #, mode='bilinear')\n",
    "\n",
    "    def encode(self, x):\n",
    "        xo = F.elu(self.conv1(x))\n",
    "        xo = F.avg_pool2d(xo, kernel_size=2, stride=2)\n",
    "        xo = F.elu(self.conv2(xo))\n",
    "        xo = F.avg_pool2d(xo, kernel_size=2, stride=2)\n",
    "        xo = F.elu(self.conv3(xo))\n",
    "        xo = xo.reshape(-1,256)\n",
    "        xo = self.fc1(xo)       \n",
    "        return xo[:, :latent_size], xo[:, latent_size:] # mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        xo = F.elu(self.fc2(z))\n",
    "        xo = xo.reshape(z.shape[0], 256, 1, 1)\n",
    "        xo = F.elu(self.conv4(xo))\n",
    "        xo = self.upsmple(xo)\n",
    "        xo = F.elu(self.conv5(xo))\n",
    "        xo = self.upsmple(xo)\n",
    "        xo = F.elu(self.conv6(xo))\n",
    "        xo = self.conv7(xo)\n",
    "        return torch.sigmoid(xo)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)#.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dHBujgp8CrYJ"
   },
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3*1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yK1E2S_ZC7GH"
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence summed over all elements in the batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # sums over all the pixels in each image and across the entire batch\n",
    "    # later we average this over all images to obtain per instance loss\n",
    "    BCE = F.binary_cross_entropy(recon_x.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
    "    \n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    KLD = 0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp())\n",
    "\n",
    "    return BCE - KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ii7qzTyxDRy6"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{}]\\tLoss: {}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7sGS55W1Sro9"
   },
   "outputs": [],
   "source": [
    "def get_avg_per_instance_ELBO(data_loader):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        param1: data_loader  - validation/test data loader\n",
    "\n",
    "    Returns:\n",
    "        Average ELBO per instance over the entire dataset \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "\n",
    "    loss = loss/len(data_loader.dataset)\n",
    "    return -loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SnjRkhvFrV_T"
   },
   "source": [
    "# Training VAE - 20 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3178
    },
    "colab_type": "code",
    "id": "4Q2gV0lsE0OD",
    "outputId": "c51a8d0b-a41e-46ec-b352-d45457d16537"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000]\tLoss: 536.7001953125\n",
      "Train Epoch: 1 [6400/50000]\tLoss: 220.7176513671875\n",
      "Train Epoch: 1 [12800/50000]\tLoss: 193.59536743164062\n",
      "Train Epoch: 1 [19200/50000]\tLoss: 171.93695068359375\n",
      "Train Epoch: 1 [25600/50000]\tLoss: 168.14175415039062\n",
      "Train Epoch: 1 [32000/50000]\tLoss: 150.896240234375\n",
      "Train Epoch: 1 [38400/50000]\tLoss: 157.8760528564453\n",
      "Train Epoch: 1 [44800/50000]\tLoss: 145.53207397460938\n",
      "====> Epoch: 1 Average loss: 184.47904995117187\n",
      "Train Epoch: 2 [0/50000]\tLoss: 146.45274353027344\n",
      "Train Epoch: 2 [6400/50000]\tLoss: 124.02973937988281\n",
      "Train Epoch: 2 [12800/50000]\tLoss: 130.306884765625\n",
      "Train Epoch: 2 [19200/50000]\tLoss: 124.01074981689453\n",
      "Train Epoch: 2 [25600/50000]\tLoss: 120.74786376953125\n",
      "Train Epoch: 2 [32000/50000]\tLoss: 126.5933609008789\n",
      "Train Epoch: 2 [38400/50000]\tLoss: 122.88883972167969\n",
      "Train Epoch: 2 [44800/50000]\tLoss: 117.56623077392578\n",
      "====> Epoch: 2 Average loss: 126.19925832519532\n",
      "Train Epoch: 3 [0/50000]\tLoss: 115.54996490478516\n",
      "Train Epoch: 3 [6400/50000]\tLoss: 118.41088104248047\n",
      "Train Epoch: 3 [12800/50000]\tLoss: 116.82450103759766\n",
      "Train Epoch: 3 [19200/50000]\tLoss: 112.69619750976562\n",
      "Train Epoch: 3 [25600/50000]\tLoss: 106.68939208984375\n",
      "Train Epoch: 3 [32000/50000]\tLoss: 106.56149291992188\n",
      "Train Epoch: 3 [38400/50000]\tLoss: 108.32494354248047\n",
      "Train Epoch: 3 [44800/50000]\tLoss: 106.27763366699219\n",
      "====> Epoch: 3 Average loss: 112.68737867919921\n",
      "Train Epoch: 4 [0/50000]\tLoss: 110.61568450927734\n",
      "Train Epoch: 4 [6400/50000]\tLoss: 103.39735412597656\n",
      "Train Epoch: 4 [12800/50000]\tLoss: 110.06708526611328\n",
      "Train Epoch: 4 [19200/50000]\tLoss: 105.57042694091797\n",
      "Train Epoch: 4 [25600/50000]\tLoss: 100.42326354980469\n",
      "Train Epoch: 4 [32000/50000]\tLoss: 105.63725280761719\n",
      "Train Epoch: 4 [38400/50000]\tLoss: 104.48180389404297\n",
      "Train Epoch: 4 [44800/50000]\tLoss: 108.02104187011719\n",
      "====> Epoch: 4 Average loss: 106.91483846191406\n",
      "Train Epoch: 5 [0/50000]\tLoss: 110.55625915527344\n",
      "Train Epoch: 5 [6400/50000]\tLoss: 108.5294189453125\n",
      "Train Epoch: 5 [12800/50000]\tLoss: 105.22650146484375\n",
      "Train Epoch: 5 [19200/50000]\tLoss: 107.37010192871094\n",
      "Train Epoch: 5 [25600/50000]\tLoss: 102.35452270507812\n",
      "Train Epoch: 5 [32000/50000]\tLoss: 99.41969299316406\n",
      "Train Epoch: 5 [38400/50000]\tLoss: 101.88367462158203\n",
      "Train Epoch: 5 [44800/50000]\tLoss: 108.21389770507812\n",
      "====> Epoch: 5 Average loss: 103.95230227050781\n",
      "Train Epoch: 6 [0/50000]\tLoss: 104.14350128173828\n",
      "Train Epoch: 6 [6400/50000]\tLoss: 102.7112045288086\n",
      "Train Epoch: 6 [12800/50000]\tLoss: 108.09583282470703\n",
      "Train Epoch: 6 [19200/50000]\tLoss: 100.10932922363281\n",
      "Train Epoch: 6 [25600/50000]\tLoss: 97.88727569580078\n",
      "Train Epoch: 6 [32000/50000]\tLoss: 106.29097747802734\n",
      "Train Epoch: 6 [38400/50000]\tLoss: 99.86453247070312\n",
      "Train Epoch: 6 [44800/50000]\tLoss: 100.21722412109375\n",
      "====> Epoch: 6 Average loss: 102.00115800292969\n",
      "Train Epoch: 7 [0/50000]\tLoss: 95.25225067138672\n",
      "Train Epoch: 7 [6400/50000]\tLoss: 102.85237121582031\n",
      "Train Epoch: 7 [12800/50000]\tLoss: 96.62998962402344\n",
      "Train Epoch: 7 [19200/50000]\tLoss: 100.43621826171875\n",
      "Train Epoch: 7 [25600/50000]\tLoss: 103.86370849609375\n",
      "Train Epoch: 7 [32000/50000]\tLoss: 95.80583953857422\n",
      "Train Epoch: 7 [38400/50000]\tLoss: 101.3628158569336\n",
      "Train Epoch: 7 [44800/50000]\tLoss: 98.50685119628906\n",
      "====> Epoch: 7 Average loss: 100.52098844970703\n",
      "Train Epoch: 8 [0/50000]\tLoss: 103.1819839477539\n",
      "Train Epoch: 8 [6400/50000]\tLoss: 100.6151123046875\n",
      "Train Epoch: 8 [12800/50000]\tLoss: 99.81006622314453\n",
      "Train Epoch: 8 [19200/50000]\tLoss: 97.2433853149414\n",
      "Train Epoch: 8 [25600/50000]\tLoss: 98.3606948852539\n",
      "Train Epoch: 8 [32000/50000]\tLoss: 102.31491088867188\n",
      "Train Epoch: 8 [38400/50000]\tLoss: 104.47453308105469\n",
      "Train Epoch: 8 [44800/50000]\tLoss: 100.28408813476562\n",
      "====> Epoch: 8 Average loss: 99.44748219970703\n",
      "Train Epoch: 9 [0/50000]\tLoss: 100.63384246826172\n",
      "Train Epoch: 9 [6400/50000]\tLoss: 97.61012268066406\n",
      "Train Epoch: 9 [12800/50000]\tLoss: 103.35408020019531\n",
      "Train Epoch: 9 [19200/50000]\tLoss: 95.05733489990234\n",
      "Train Epoch: 9 [25600/50000]\tLoss: 96.374755859375\n",
      "Train Epoch: 9 [32000/50000]\tLoss: 97.57178497314453\n",
      "Train Epoch: 9 [38400/50000]\tLoss: 99.52655029296875\n",
      "Train Epoch: 9 [44800/50000]\tLoss: 93.92604064941406\n",
      "====> Epoch: 9 Average loss: 98.5142929736328\n",
      "Train Epoch: 10 [0/50000]\tLoss: 102.15038299560547\n",
      "Train Epoch: 10 [6400/50000]\tLoss: 97.68553161621094\n",
      "Train Epoch: 10 [12800/50000]\tLoss: 96.07262420654297\n",
      "Train Epoch: 10 [19200/50000]\tLoss: 102.64293670654297\n",
      "Train Epoch: 10 [25600/50000]\tLoss: 102.16189575195312\n",
      "Train Epoch: 10 [32000/50000]\tLoss: 99.98849487304688\n",
      "Train Epoch: 10 [38400/50000]\tLoss: 90.69587707519531\n",
      "Train Epoch: 10 [44800/50000]\tLoss: 94.942138671875\n",
      "====> Epoch: 10 Average loss: 97.79952147949218\n",
      "Train Epoch: 11 [0/50000]\tLoss: 99.99852752685547\n",
      "Train Epoch: 11 [6400/50000]\tLoss: 97.62261962890625\n",
      "Train Epoch: 11 [12800/50000]\tLoss: 101.32764434814453\n",
      "Train Epoch: 11 [19200/50000]\tLoss: 100.8727035522461\n",
      "Train Epoch: 11 [25600/50000]\tLoss: 94.95201873779297\n",
      "Train Epoch: 11 [32000/50000]\tLoss: 93.42790985107422\n",
      "Train Epoch: 11 [38400/50000]\tLoss: 92.77306365966797\n",
      "Train Epoch: 11 [44800/50000]\tLoss: 89.96063232421875\n",
      "====> Epoch: 11 Average loss: 97.18277021972656\n",
      "Train Epoch: 12 [0/50000]\tLoss: 94.15922546386719\n",
      "Train Epoch: 12 [6400/50000]\tLoss: 96.25369262695312\n",
      "Train Epoch: 12 [12800/50000]\tLoss: 102.61495971679688\n",
      "Train Epoch: 12 [19200/50000]\tLoss: 93.20831298828125\n",
      "Train Epoch: 12 [25600/50000]\tLoss: 96.98296356201172\n",
      "Train Epoch: 12 [32000/50000]\tLoss: 103.13868713378906\n",
      "Train Epoch: 12 [38400/50000]\tLoss: 102.28143310546875\n",
      "Train Epoch: 12 [44800/50000]\tLoss: 93.14083862304688\n",
      "====> Epoch: 12 Average loss: 96.60126958984375\n",
      "Train Epoch: 13 [0/50000]\tLoss: 95.40950012207031\n",
      "Train Epoch: 13 [6400/50000]\tLoss: 94.19001770019531\n",
      "Train Epoch: 13 [12800/50000]\tLoss: 101.81645965576172\n",
      "Train Epoch: 13 [19200/50000]\tLoss: 93.28509521484375\n",
      "Train Epoch: 13 [25600/50000]\tLoss: 94.62446594238281\n",
      "Train Epoch: 13 [32000/50000]\tLoss: 97.25074768066406\n",
      "Train Epoch: 13 [38400/50000]\tLoss: 90.78651428222656\n",
      "Train Epoch: 13 [44800/50000]\tLoss: 101.00914001464844\n",
      "====> Epoch: 13 Average loss: 96.1475276538086\n",
      "Train Epoch: 14 [0/50000]\tLoss: 100.91851806640625\n",
      "Train Epoch: 14 [6400/50000]\tLoss: 95.60309600830078\n",
      "Train Epoch: 14 [12800/50000]\tLoss: 90.93525695800781\n",
      "Train Epoch: 14 [19200/50000]\tLoss: 95.97144317626953\n",
      "Train Epoch: 14 [25600/50000]\tLoss: 100.56182861328125\n",
      "Train Epoch: 14 [32000/50000]\tLoss: 99.31167602539062\n",
      "Train Epoch: 14 [38400/50000]\tLoss: 95.38703918457031\n",
      "Train Epoch: 14 [44800/50000]\tLoss: 96.86575317382812\n",
      "====> Epoch: 14 Average loss: 95.63657587890626\n",
      "Train Epoch: 15 [0/50000]\tLoss: 98.57672882080078\n",
      "Train Epoch: 15 [6400/50000]\tLoss: 98.50220489501953\n",
      "Train Epoch: 15 [12800/50000]\tLoss: 98.84877014160156\n",
      "Train Epoch: 15 [19200/50000]\tLoss: 94.41437530517578\n",
      "Train Epoch: 15 [25600/50000]\tLoss: 93.93130493164062\n",
      "Train Epoch: 15 [32000/50000]\tLoss: 100.56885528564453\n",
      "Train Epoch: 15 [38400/50000]\tLoss: 96.75128173828125\n",
      "Train Epoch: 15 [44800/50000]\tLoss: 93.81991577148438\n",
      "====> Epoch: 15 Average loss: 95.27752162597656\n",
      "Train Epoch: 16 [0/50000]\tLoss: 96.84318542480469\n",
      "Train Epoch: 16 [6400/50000]\tLoss: 94.20489501953125\n",
      "Train Epoch: 16 [12800/50000]\tLoss: 88.93770599365234\n",
      "Train Epoch: 16 [19200/50000]\tLoss: 96.85267639160156\n",
      "Train Epoch: 16 [25600/50000]\tLoss: 89.28492736816406\n",
      "Train Epoch: 16 [32000/50000]\tLoss: 99.6402587890625\n",
      "Train Epoch: 16 [38400/50000]\tLoss: 97.67587280273438\n",
      "Train Epoch: 16 [44800/50000]\tLoss: 99.9782485961914\n",
      "====> Epoch: 16 Average loss: 94.94571291503907\n",
      "Train Epoch: 17 [0/50000]\tLoss: 94.54561614990234\n",
      "Train Epoch: 17 [6400/50000]\tLoss: 96.41292572021484\n",
      "Train Epoch: 17 [12800/50000]\tLoss: 96.18717193603516\n",
      "Train Epoch: 17 [19200/50000]\tLoss: 90.97410583496094\n",
      "Train Epoch: 17 [25600/50000]\tLoss: 95.21098327636719\n",
      "Train Epoch: 17 [32000/50000]\tLoss: 98.1252670288086\n",
      "Train Epoch: 17 [38400/50000]\tLoss: 92.45774841308594\n",
      "Train Epoch: 17 [44800/50000]\tLoss: 96.91964721679688\n",
      "====> Epoch: 17 Average loss: 94.55363986328125\n",
      "Train Epoch: 18 [0/50000]\tLoss: 97.23448944091797\n",
      "Train Epoch: 18 [6400/50000]\tLoss: 96.11405181884766\n",
      "Train Epoch: 18 [12800/50000]\tLoss: 92.22161865234375\n",
      "Train Epoch: 18 [19200/50000]\tLoss: 93.9126968383789\n",
      "Train Epoch: 18 [25600/50000]\tLoss: 98.88880920410156\n",
      "Train Epoch: 18 [32000/50000]\tLoss: 98.94950103759766\n",
      "Train Epoch: 18 [38400/50000]\tLoss: 93.57276916503906\n",
      "Train Epoch: 18 [44800/50000]\tLoss: 94.48368835449219\n",
      "====> Epoch: 18 Average loss: 94.23921942138672\n",
      "Train Epoch: 19 [0/50000]\tLoss: 97.7276611328125\n",
      "Train Epoch: 19 [6400/50000]\tLoss: 90.79278564453125\n",
      "Train Epoch: 19 [12800/50000]\tLoss: 94.96778869628906\n",
      "Train Epoch: 19 [19200/50000]\tLoss: 94.70367431640625\n",
      "Train Epoch: 19 [25600/50000]\tLoss: 95.4384994506836\n",
      "Train Epoch: 19 [32000/50000]\tLoss: 90.02162170410156\n",
      "Train Epoch: 19 [38400/50000]\tLoss: 95.23184967041016\n",
      "Train Epoch: 19 [44800/50000]\tLoss: 94.88630676269531\n",
      "====> Epoch: 19 Average loss: 94.06791859863282\n",
      "Train Epoch: 20 [0/50000]\tLoss: 97.22611999511719\n",
      "Train Epoch: 20 [6400/50000]\tLoss: 90.91141510009766\n",
      "Train Epoch: 20 [12800/50000]\tLoss: 90.48937225341797\n",
      "Train Epoch: 20 [19200/50000]\tLoss: 93.91841125488281\n",
      "Train Epoch: 20 [25600/50000]\tLoss: 92.94747924804688\n",
      "Train Epoch: 20 [32000/50000]\tLoss: 92.1898193359375\n",
      "Train Epoch: 20 [38400/50000]\tLoss: 96.38092803955078\n",
      "Train Epoch: 20 [44800/50000]\tLoss: 95.97161102294922\n",
      "====> Epoch: 20 Average loss: 93.73590671386718\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "if __name__ == \"__main__\":\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y6k6LEXzsdgZ"
   },
   "source": [
    "## Compute average per instance ELBO for Validation/Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "AzCqyu6ol0FQ",
    "outputId": "a0fd8691-6854-4d1c-a2ec-7853898516c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid - average per-instance ELBO: -94.25169111328125 \n"
     ]
    }
   ],
   "source": [
    "ELBO_valid = get_avg_per_instance_ELBO(valid_loader)\n",
    "print('Valid - average per-instance ELBO: {} '.format(ELBO_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "HeQmKMrKurCG",
    "outputId": "f66c3a7a-77cd-4f99-ea4b-ed661d6c4fe5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  - average per-instance ELBO: -93.62225546875 \n"
     ]
    }
   ],
   "source": [
    "ELBO_test = get_avg_per_instance_ELBO(test_loader)\n",
    "print('Test  - average per-instance ELBO: {} '.format(ELBO_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDx-kb2ynFn8"
   },
   "outputs": [],
   "source": [
    "# saves trained model\n",
    "# torch.save(model, path+'vae_torch.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_1xZXtkAFWI"
   },
   "source": [
    "# Importance Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4ekrXHbLNXS"
   },
   "outputs": [],
   "source": [
    "model = torch.load(path+'vae_torch.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5gSnJKK_TyU"
   },
   "outputs": [],
   "source": [
    "def get_samples(K, mu, sig):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        param1: int K  - number of samples\n",
    "        param2: array mu (M,L) - mean from the encoder network\n",
    "        param3: array sig (M,L) - std from encoder network\n",
    "\n",
    "    Returns:\n",
    "        samples of size (M,K,L) \n",
    "        M: batchsize, K: num samples, L: latent size\n",
    "    \"\"\"\n",
    "    return np.array([stats.norm.rvs(loc=mi, scale=si, size = K, random_state=None) \n",
    "                  for mi,si in zip(mu,sig)]).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZfD8gLfe-vbw"
   },
   "outputs": [],
   "source": [
    "def get_log_px(model, data, samples):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        param1: trained torch model\n",
    "        param2: array data (M,D)\n",
    "        param3: array samples (M,K,L)\n",
    "\n",
    "    Returns:\n",
    "        An array of log p(x) for the batch of size M\n",
    "        (log p(x_1), ..., log p(x_M))\n",
    "    \"\"\"\n",
    "\n",
    "    K = samples.shape[1]\n",
    "    mu, logvar = model.encode(data)\n",
    "    mu = mu.data.numpy()\n",
    "    sig = torch.sqrt(torch.exp(logvar)).data.numpy()\n",
    "    log_px_batch = []\n",
    "\n",
    "    for i,(x,z,m,s) in enumerate(zip(data,samples,mu,sig)): # iterate through a batch\n",
    "\n",
    "        ### q(z|x): stores log_qzx for numerical stability\n",
    "        qzx = stats.norm(loc=m,scale=s).pdf(z)\n",
    "        log_qzx = np.sum(np.log(qzx), axis=1)\n",
    "\n",
    "        ### p(z): stores log_pz for numerical stability\n",
    "        pz = stats.norm.pdf(z)\n",
    "        log_pz = np.sum(np.log(pz), axis=1)\n",
    "\n",
    "        ### p(x|z): stores log_pxz for numerical stability\n",
    "        z = z.reshape(K,1,-1)\n",
    "        x_reconst = model.decode(torch.Tensor(z))\n",
    "        x_reconst = x_reconst.data.numpy()\n",
    "        # clip probabilities close to zero/one to prevent exception\n",
    "        lower_clip = np.finfo(float).eps\n",
    "        upper_clip = 1 - np.finfo(float).eps\n",
    "        x_reconst = np.clip(x_reconst, lower_clip, upper_clip)\n",
    "        x_reconst = torch.Tensor(x_reconst)\n",
    "        log_pxz = np.sum((x * (x_reconst).log() + (1 - x) * (1 - x_reconst).log()).reshape(K,-1).data.numpy(),axis=1)\n",
    "\n",
    "        # log p(x) = log(1/K) + log(sum(exp(log p(x|z)+log p(z)-log q(z|x))))\n",
    "        terms = log_pxz + log_pz - log_qzx\n",
    "        log_px = -np.log(K) + np.log(np.sum(np.exp(terms))) \n",
    "\n",
    "        log_px_batch.append(log_px)\n",
    "\n",
    "    return log_px_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "zsFqO-Abs9aG",
    "outputId": "745f56b9-bece-4be8-c656-609dfa88348e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-85.53463030044941\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(valid_loader):\n",
    "  break\n",
    "  \n",
    "mu, logvar = model.encode(data)\n",
    "mu = mu.data.numpy()\n",
    "sig = torch.sqrt(torch.exp(logvar)).data.numpy()\n",
    "\n",
    "# obtains samples (M,K,L) => (64, 200, 100)\n",
    "samples = np.array([get_samples(200, mi, si) for (mi,si) in zip(mu,sig)]) \n",
    "\n",
    "log_px_batch = get_log_px(model,data,samples)\n",
    "print(np.mean(log_px_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ov3gotXZBZWE"
   },
   "source": [
    "## Compute average log-likelihood using importance sampling on Validation/Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "coj9UPBaBX-L"
   },
   "outputs": [],
   "source": [
    "def get_log_px_dataset(data_loader, K):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        param1: data loader for the dataset\n",
    "        param2: K num samples\n",
    "\n",
    "    Returns:\n",
    "        An array of log p(x) for the batch of size M\n",
    "        (log p(x_1), ..., log p(x_M))\n",
    "    \"\"\"\n",
    "\n",
    "    log_px_all = []\n",
    "    for i, data in enumerate(data_loader):\n",
    "        data = data.to(device)\n",
    "        mu, logvar = model.encode(data)\n",
    "        mu = mu.data.numpy()\n",
    "        sig = torch.sqrt(torch.exp(logvar)).data.numpy()\n",
    "\n",
    "        # obtains samples (M,K,L) => (64, 200, 100)\n",
    "        samples = np.array([get_samples(K, mi, si) for (mi,si) in zip(mu,sig)]) \n",
    "\n",
    "        log_px_batch = get_log_px(model,data,samples)\n",
    "        log_px_all.extend(log_px_batch)\n",
    "\n",
    "    return log_px_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "1CyZZ9uQusPU",
    "outputId": "cc6ea52c-9500-4b59-aded-370998c5e55d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - average log-likelihood estimate: -88.73825199924798 \n"
     ]
    }
   ],
   "source": [
    "log_px_valid = get_log_px_dataset(valid_loader, 200)\n",
    "print('Validation - average log-likelihood estimate: {} '.format(np.mean(log_px_valid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "FpxCUfb80vP-",
    "outputId": "503f4cb8-1010-4987-9b20-328166b53a28"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - average log-likelihood estimate: -88.07301650262484 \n"
     ]
    }
   ],
   "source": [
    "log_px_test  = get_log_px_dataset(test_loader, 200)\n",
    "print('Test - average log-likelihood estimate: {} '.format(np.mean(log_px_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNzIFqESa5Gc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VAE_with_Sampling.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
